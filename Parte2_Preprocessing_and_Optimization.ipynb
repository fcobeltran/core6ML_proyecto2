{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Proyecto 2 - Parte II: Preprocesamiento y Optimización\n",
        "\n",
        "## Sistema Inteligente de Recomendación de Inversiones\n",
        "### Clasificación de Señales de Trading (Buy/Hold/Sell)\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivos de la Parte II\n",
        "\n",
        "1. **Preprocesamiento de Datos:**\n",
        "   - Limpieza de datos (valores nulos, outliers)\n",
        "   - Transformación de columnas con ColumnTransformer\n",
        "   - Creación de indicadores técnicos (features)\n",
        "   - Escalado y normalización\n",
        "   - Pipelines automatizados\n",
        "\n",
        "2. **Selección de Técnica de ML:**\n",
        "   - Entrenamiento de múltiples modelos\n",
        "   - Evaluación con validación cruzada\n",
        "   - Comparación de rendimiento\n",
        "\n",
        "3. **Optimización de Hiperparámetros:**\n",
        "   - GridSearchCV\n",
        "   - RandomizedSearchCV\n",
        "   - Optuna (optimización bayesiana)\n",
        "\n",
        "4. **Evaluación Final:**\n",
        "   - Métricas de rendimiento\n",
        "   - Análisis de resultados\n",
        "   - Recomendaciones\n",
        "\n",
        "---\n",
        "\n",
        "### Problema Seleccionado: Clasificación de Señales de Trading\n",
        "\n",
        "**Target:** Predecir señales de inversión basadas en indicadores técnicos\n",
        "- **BUY (0):** Momento favorable para comprar \n",
        "- **HOLD (1):** Mantener posición actual\n",
        "- **SELL (2):** Momento favorable para vender\n",
        "\n",
        "**Justificación:** Este problema es altamente práctico para inversores y combina análisis técnico con machine learning para tomar decisiones informadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importación de librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Modelos de ML\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Métricas\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "\n",
        "# Optimización avanzada\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"✅ Optuna disponible para optimización bayesiana\")\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"⚠️ Optuna no disponible. Instalando...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"✅ Optuna instalado exitosamente\")\n",
        "\n",
        "# Configuración\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"🚀 Librerías importadas exitosamente\")\n",
        "print(\"📊 Iniciando Parte II: Preprocesamiento y Optimización\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga y Análisis Inicial de Datos\n",
        "\n",
        "Cargaremos el dataset financiero combinado y realizaremos un análisis inicial para entender la estructura de los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset combinado\n",
        "print(\"📈 Cargando dataset financiero combinado...\")\n",
        "df = pd.read_csv('selected_dataset/financial_data_combined.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "print(f\"✅ Dataset cargado exitosamente\")\n",
        "print(f\"📊 Forma del dataset: {df.shape}\")\n",
        "print(f\"🗓️ Período: {df['Date'].min()} a {df['Date'].max()}\")\n",
        "print(f\"🏢 Acciones: {', '.join(df['Stock'].unique())}\")\n",
        "print(f\"🏭 Sectores: {', '.join(df['Sector'].unique())}\")\n",
        "\n",
        "# Información básica del dataset\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INFORMACIÓN DEL DATASET\")\n",
        "print(\"=\"*50)\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ESTADÍSTICAS DESCRIPTIVAS\")\n",
        "print(\"=\"*50)\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALORES NULOS\")\n",
        "print(\"=\"*50)\n",
        "null_counts = df.isnull().sum()\n",
        "null_percentages = (null_counts / len(df)) * 100\n",
        "null_summary = pd.DataFrame({\n",
        "    'Valores Nulos': null_counts,\n",
        "    'Porcentaje': null_percentages\n",
        "})\n",
        "print(null_summary[null_summary['Valores Nulos'] > 0])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRIMERAS 5 FILAS\")\n",
        "print(\"=\"*50)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Feature Engineering - Creación de Indicadores Técnicos\n",
        "\n",
        "Crearemos indicadores técnicos que servirán como features para nuestro modelo de clasificación de señales de trading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Crea indicadores técnicos para análisis de trading\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    print(\"🔧 Creando indicadores técnicos...\")\n",
        "    \n",
        "    # Agrupar por acción para calcular indicadores por separado\n",
        "    dfs_processed = []\n",
        "    \n",
        "    for stock in df['Stock'].unique():\n",
        "        stock_df = df[df['Stock'] == stock].copy()\n",
        "        stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
        "        \n",
        "        # ========== INDICADORES DE PRECIO ==========\n",
        "        \n",
        "        # Retornos\n",
        "        stock_df['Daily_Return'] = stock_df['Adjusted Close'].pct_change()\n",
        "        stock_df['Price_Change'] = stock_df['Adjusted Close'].diff()\n",
        "        \n",
        "        # Promedios móviles\n",
        "        stock_df['MA_5'] = stock_df['Adjusted Close'].rolling(window=5).mean()\n",
        "        stock_df['MA_10'] = stock_df['Adjusted Close'].rolling(window=10).mean()\n",
        "        stock_df['MA_20'] = stock_df['Adjusted Close'].rolling(window=20).mean()\n",
        "        stock_df['MA_50'] = stock_df['Adjusted Close'].rolling(window=50, min_periods=30).mean()\n",
        "        \n",
        "        # Exponential Moving Averages\n",
        "        stock_df['EMA_12'] = stock_df['Adjusted Close'].ewm(span=12).mean()\n",
        "        stock_df['EMA_26'] = stock_df['Adjusted Close'].ewm(span=26).mean()\n",
        "        \n",
        "        # ========== RSI (Relative Strength Index) ==========\n",
        "        delta = stock_df['Adjusted Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        stock_df['RSI'] = 100 - (100 / (1 + rs))\n",
        "        \n",
        "        # ========== MACD ==========\n",
        "        stock_df['MACD'] = stock_df['EMA_12'] - stock_df['EMA_26']\n",
        "        stock_df['MACD_Signal'] = stock_df['MACD'].ewm(span=9).mean()\n",
        "        stock_df['MACD_Histogram'] = stock_df['MACD'] - stock_df['MACD_Signal']\n",
        "        \n",
        "        # ========== BOLLINGER BANDS ==========\n",
        "        stock_df['BB_Middle'] = stock_df['MA_20']\n",
        "        bb_std = stock_df['Adjusted Close'].rolling(window=20).std()\n",
        "        stock_df['BB_Upper'] = stock_df['BB_Middle'] + (bb_std * 2)\n",
        "        stock_df['BB_Lower'] = stock_df['BB_Middle'] - (bb_std * 2)\n",
        "        stock_df['BB_Width'] = stock_df['BB_Upper'] - stock_df['BB_Lower']\n",
        "        stock_df['BB_Position'] = (stock_df['Adjusted Close'] - stock_df['BB_Lower']) / stock_df['BB_Width']\n",
        "        \n",
        "        # ========== STOCHASTIC OSCILLATOR ==========\n",
        "        high_14 = stock_df['High'].rolling(window=14).max()\n",
        "        low_14 = stock_df['Low'].rolling(window=14).min()\n",
        "        stock_df['Stoch_K'] = 100 * (stock_df['Close'] - low_14) / (high_14 - low_14)\n",
        "        stock_df['Stoch_D'] = stock_df['Stoch_K'].rolling(window=3).mean()\n",
        "        \n",
        "        # ========== WILLIAMS %R ==========\n",
        "        stock_df['Williams_R'] = -100 * (high_14 - stock_df['Close']) / (high_14 - low_14)\n",
        "        \n",
        "        # ========== INDICADORES DE VOLUMEN ==========\n",
        "        stock_df['Volume_MA'] = stock_df['Volume'].rolling(window=20).mean()\n",
        "        stock_df['Volume_Ratio'] = stock_df['Volume'] / stock_df['Volume_MA']\n",
        "        \n",
        "        # Money Flow Index (MFI)\n",
        "        typical_price = (stock_df['High'] + stock_df['Low'] + stock_df['Close']) / 3\n",
        "        money_flow = typical_price * stock_df['Volume']\n",
        "        \n",
        "        positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0).rolling(window=14).sum()\n",
        "        negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0).rolling(window=14).sum()\n",
        "        \n",
        "        mfi_ratio = positive_flow / negative_flow\n",
        "        stock_df['MFI'] = 100 - (100 / (1 + mfi_ratio))\n",
        "        \n",
        "        # ========== INDICADORES DE VOLATILIDAD ==========\n",
        "        stock_df['Volatility'] = stock_df['Daily_Return'].rolling(window=20).std()\n",
        "        stock_df['ATR'] = ((stock_df['High'] - stock_df['Low']).rolling(window=14).mean())\n",
        "        \n",
        "        # ========== RATIOS DE PRECIO ==========\n",
        "        stock_df['Price_to_MA20'] = stock_df['Adjusted Close'] / stock_df['MA_20']\n",
        "        stock_df['MA5_to_MA20'] = stock_df['MA_5'] / stock_df['MA_20']\n",
        "        stock_df['High_Low_Ratio'] = stock_df['High'] / stock_df['Low']\n",
        "        \n",
        "        dfs_processed.append(stock_df)\n",
        "        print(f\"   ✅ {stock}: {len([col for col in stock_df.columns if col not in df.columns])} nuevos indicadores\")\n",
        "    \n",
        "    # Combinar todos los dataframes procesados\n",
        "    df_combined = pd.concat(dfs_processed, ignore_index=True)\n",
        "    \n",
        "    print(f\"🎯 Feature Engineering completado\")\n",
        "    print(f\"📊 Nuevas columnas creadas: {len(df_combined.columns) - len(df.columns)}\")\n",
        "    print(f\"📈 Dataset final: {df_combined.shape}\")\n",
        "    \n",
        "    return df_combined\n",
        "\n",
        "# Aplicar feature engineering\n",
        "df_features = create_technical_indicators(df)\n",
        "\n",
        "# Mostrar las nuevas columnas creadas\n",
        "new_columns = [col for col in df_features.columns if col not in df.columns]\n",
        "print(f\"\\n📋 Indicadores técnicos creados ({len(new_columns)}):\")\n",
        "for i, col in enumerate(new_columns, 1):\n",
        "    print(f\"   {i:2d}. {col}\")\n",
        "\n",
        "# Verificar valores nulos después del feature engineering\n",
        "print(f\"\\n⚠️ Valores nulos después del feature engineering:\")\n",
        "null_summary_new = df_features.isnull().sum()\n",
        "print(null_summary_new[null_summary_new > 0].head(10))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Creación de Variable Objetivo (Target)\n",
        "\n",
        "Crearemos las etiquetas de trading basadas en una combinación de indicadores técnicos y retornos futuros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_trading_signals(df, forward_days=5, rsi_buy=30, rsi_sell=70):\n",
        "    \"\"\"\n",
        "    Crea señales de trading basadas en indicadores técnicos y retornos futuros\n",
        "    \n",
        "    Parámetros:\n",
        "    - forward_days: días hacia adelante para calcular retornos futuros\n",
        "    - rsi_buy: nivel RSI para señal de compra\n",
        "    - rsi_sell: nivel RSI para señal de venta\n",
        "    \n",
        "    Señales:\n",
        "    - 0: BUY - Condiciones favorables para comprar\n",
        "    - 1: HOLD - Mantener posición actual  \n",
        "    - 2: SELL - Condiciones favorables para vender\n",
        "    \"\"\"\n",
        "    \n",
        "    df = df.copy()\n",
        "    print(f\"🎯 Creando señales de trading...\")\n",
        "    print(f\"   📊 RSI Buy threshold: {rsi_buy}\")\n",
        "    print(f\"   📊 RSI Sell threshold: {rsi_sell}\")\n",
        "    print(f\"   📅 Forward days: {forward_days}\")\n",
        "    \n",
        "    # Procesar cada acción por separado\n",
        "    dfs_with_signals = []\n",
        "    \n",
        "    for stock in df['Stock'].unique():\n",
        "        stock_df = df[df['Stock'] == stock].copy()\n",
        "        stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
        "        \n",
        "        # Calcular retorno futuro\n",
        "        stock_df['Future_Return'] = stock_df['Adjusted Close'].pct_change(periods=forward_days).shift(-forward_days)\n",
        "        \n",
        "        # Inicializar señales como HOLD (1)\n",
        "        stock_df['Trading_Signal'] = 1\n",
        "        \n",
        "        # Condiciones para BUY (0)\n",
        "        buy_conditions = (\n",
        "            (stock_df['RSI'] < rsi_buy) &  # RSI oversold\n",
        "            (stock_df['BB_Position'] < 0.2) &  # Precio cerca del límite inferior de Bollinger\n",
        "            (stock_df['MACD'] > stock_df['MACD_Signal']) &  # MACD bullish\n",
        "            (stock_df['Stoch_K'] < 20) &  # Stochastic oversold\n",
        "            (stock_df['Future_Return'] > 0.02)  # Retorno futuro positivo > 2%\n",
        "        )\n",
        "        \n",
        "        # Condiciones para SELL (2)\n",
        "        sell_conditions = (\n",
        "            (stock_df['RSI'] > rsi_sell) &  # RSI overbought\n",
        "            (stock_df['BB_Position'] > 0.8) &  # Precio cerca del límite superior de Bollinger\n",
        "            (stock_df['MACD'] < stock_df['MACD_Signal']) &  # MACD bearish\n",
        "            (stock_df['Stoch_K'] > 80) &  # Stochastic overbought\n",
        "            (stock_df['Future_Return'] < -0.02)  # Retorno futuro negativo > -2%\n",
        "        )\n",
        "        \n",
        "        # Aplicar las condiciones\n",
        "        stock_df.loc[buy_conditions, 'Trading_Signal'] = 0  # BUY\n",
        "        stock_df.loc[sell_conditions, 'Trading_Signal'] = 2  # SELL\n",
        "        \n",
        "        dfs_with_signals.append(stock_df)\n",
        "        \n",
        "        # Estadísticas por acción\n",
        "        signal_counts = stock_df['Trading_Signal'].value_counts().sort_index()\n",
        "        print(f\"   📈 {stock}: BUY={signal_counts.get(0, 0)}, HOLD={signal_counts.get(1, 0)}, SELL={signal_counts.get(2, 0)}\")\n",
        "    \n",
        "    # Combinar todas las acciones\n",
        "    df_with_signals = pd.concat(dfs_with_signals, ignore_index=True)\n",
        "    \n",
        "    # Estadísticas generales\n",
        "    print(f\"\\n📊 Distribución de señales general:\")\n",
        "    signal_distribution = df_with_signals['Trading_Signal'].value_counts().sort_index()\n",
        "    total_signals = len(df_with_signals.dropna(subset=['Trading_Signal']))\n",
        "    \n",
        "    for signal, count in signal_distribution.items():\n",
        "        signal_name = ['BUY', 'HOLD', 'SELL'][signal]\n",
        "        percentage = (count / total_signals) * 100\n",
        "        print(f\"   {signal_name} ({signal}): {count:,} ({percentage:.1f}%)\")\n",
        "    \n",
        "    return df_with_signals\n",
        "\n",
        "# Crear señales de trading\n",
        "df_with_signals = create_trading_signals(df_features)\n",
        "\n",
        "# Visualizar la distribución de señales\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Subplot 1: Distribución general\n",
        "plt.subplot(1, 2, 1)\n",
        "signal_counts = df_with_signals['Trading_Signal'].value_counts().sort_index()\n",
        "labels = ['BUY', 'HOLD', 'SELL']\n",
        "colors = ['green', 'orange', 'red']\n",
        "plt.pie(signal_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribución de Señales de Trading')\n",
        "\n",
        "# Subplot 2: Distribución por acción\n",
        "plt.subplot(1, 2, 2)\n",
        "signal_by_stock = df_with_signals.groupby(['Stock', 'Trading_Signal']).size().unstack(fill_value=0)\n",
        "signal_by_stock.plot(kind='bar', color=colors, alpha=0.7)\n",
        "plt.title('Señales por Acción')\n",
        "plt.xlabel('Acción')\n",
        "plt.ylabel('Cantidad de Señales')\n",
        "plt.legend(['BUY', 'HOLD', 'SELL'])\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ Variable objetivo creada exitosamente\")\n",
        "print(f\"📊 Dataset con señales: {df_with_signals.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Preprocesamiento de Datos\n",
        "\n",
        "Ahora aplicaremos técnicas de preprocesamiento incluyendo limpieza de datos, manejo de valores nulos y outliers, y creación de pipelines automatizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PREPARACIÓN DE DATOS PARA MACHINE LEARNING\n",
        "# ============================================================\n",
        "\n",
        "print(\"🔧 Iniciando preprocesamiento de datos...\")\n",
        "\n",
        "# 1. LIMPIEZA INICIAL - Remover filas con valores nulos en la variable objetivo\n",
        "print(\"\\n1️⃣ Limpieza inicial...\")\n",
        "data_clean = df_with_signals.dropna(subset=['Trading_Signal']).copy()\n",
        "print(f\"   Datos después de remover NaN en Trading_Signal: {data_clean.shape}\")\n",
        "\n",
        "# 2. SELECCIÓN DE FEATURES PARA EL MODELO\n",
        "print(\"\\n2️⃣ Selección de features...\")\n",
        "\n",
        "# Features técnicos (indicadores)\n",
        "technical_features = [\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'MACD_Histogram',\n",
        "    'BB_Position', 'BB_Width', 'Stoch_K', 'Stoch_D', 'Williams_R',\n",
        "    'MFI', 'Volatility', 'ATR', 'Volume_Ratio',\n",
        "    'Price_to_MA20', 'MA5_to_MA20', 'High_Low_Ratio',\n",
        "    'Daily_Return', 'Price_Change'\n",
        "]\n",
        "\n",
        "# Features categóricas\n",
        "categorical_features = ['Stock', 'Sector', 'Market']\n",
        "\n",
        "# Features a excluir (identificadores, fechas, etc.)\n",
        "exclude_features = ['Date', 'Future_Return', 'Trading_Signal']\n",
        "\n",
        "# Verificar que las features existen\n",
        "available_technical = [f for f in technical_features if f in data_clean.columns]\n",
        "missing_technical = [f for f in technical_features if f not in data_clean.columns]\n",
        "\n",
        "print(f\"   ✅ Features técnicos disponibles: {len(available_technical)}\")\n",
        "print(f\"   ⚠️ Features técnicos faltantes: {len(missing_technical)}\")\n",
        "if missing_technical:\n",
        "    print(f\"      Faltantes: {missing_technical}\")\n",
        "\n",
        "# 3. ANÁLISIS DE VALORES NULOS\n",
        "print(\"\\n3️⃣ Análisis de valores nulos...\")\n",
        "null_analysis = data_clean[available_technical + categorical_features].isnull().sum()\n",
        "null_features = null_analysis[null_analysis > 0]\n",
        "\n",
        "if len(null_features) > 0:\n",
        "    print(\"   Columnas con valores nulos:\")\n",
        "    for col, count in null_features.items():\n",
        "        percentage = (count / len(data_clean)) * 100\n",
        "        print(f\"      {col}: {count} ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"   ✅ No hay valores nulos en las features seleccionadas\")\n",
        "\n",
        "# 4. DETECCIÓN DE OUTLIERS\n",
        "print(\"\\n4️⃣ Detección de outliers...\")\n",
        "\n",
        "def detect_outliers_iqr(data, column, factor=1.5):\n",
        "    \"\"\"Detecta outliers usando el método IQR\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - factor * IQR\n",
        "    upper_bound = Q3 + factor * IQR\n",
        "    \n",
        "    outliers = ((data[column] < lower_bound) | (data[column] > upper_bound))\n",
        "    return outliers.sum(), lower_bound, upper_bound\n",
        "\n",
        "outlier_summary = []\n",
        "for feature in available_technical:\n",
        "    if data_clean[feature].dtype in ['float64', 'int64']:\n",
        "        outlier_count, lower, upper = detect_outliers_iqr(data_clean, feature)\n",
        "        outlier_percentage = (outlier_count / len(data_clean)) * 100\n",
        "        outlier_summary.append({\n",
        "            'Feature': feature,\n",
        "            'Outliers': outlier_count,\n",
        "            'Percentage': outlier_percentage,\n",
        "            'Lower_Bound': lower,\n",
        "            'Upper_Bound': upper\n",
        "        })\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary)\n",
        "outlier_df = outlier_df.sort_values('Percentage', ascending=False)\n",
        "\n",
        "print(\"   Top 10 features con más outliers:\")\n",
        "print(outlier_df.head(10)[['Feature', 'Outliers', 'Percentage']].to_string(index=False))\n",
        "\n",
        "# 5. PREPARACIÓN DE DATASETS X y y\n",
        "print(\"\\n5️⃣ Preparación de datasets X y y...\")\n",
        "\n",
        "# Seleccionar solo las filas con todas las features disponibles\n",
        "features_to_use = available_technical + categorical_features\n",
        "X = data_clean[features_to_use].copy()\n",
        "y = data_clean['Trading_Signal'].copy()\n",
        "\n",
        "# Remover filas con valores nulos en X\n",
        "mask_complete = X.notna().all(axis=1)\n",
        "X = X[mask_complete]\n",
        "y = y[mask_complete]\n",
        "\n",
        "print(f\"   📊 Shape final de X: {X.shape}\")\n",
        "print(f\"   🎯 Shape final de y: {y.shape}\")\n",
        "print(f\"   📈 Distribución de clases:\")\n",
        "\n",
        "class_distribution = y.value_counts().sort_index()\n",
        "for class_val, count in class_distribution.items():\n",
        "    class_name = ['BUY', 'HOLD', 'SELL'][int(class_val)]\n",
        "    percentage = (count / len(y)) * 100\n",
        "    print(f\"      {class_name} ({class_val}): {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "# 6. DIVISIÓN TEMPORAL DE DATOS\n",
        "print(\"\\n6️⃣ División temporal de datos...\")\n",
        "\n",
        "# Obtener las fechas correspondientes\n",
        "dates_complete = data_clean[mask_complete]['Date'].reset_index(drop=True)\n",
        "\n",
        "# Ordenar por fecha para división temporal\n",
        "sort_idx = dates_complete.argsort()\n",
        "X_sorted = X.iloc[sort_idx].reset_index(drop=True)\n",
        "y_sorted = y.iloc[sort_idx].reset_index(drop=True)\n",
        "dates_sorted = dates_complete.iloc[sort_idx].reset_index(drop=True)\n",
        "\n",
        "# División temporal 80/20\n",
        "split_index = int(0.8 * len(X_sorted))\n",
        "\n",
        "X_train = X_sorted.iloc[:split_index]\n",
        "X_test = X_sorted.iloc[split_index:]\n",
        "y_train = y_sorted.iloc[:split_index]\n",
        "y_test = y_sorted.iloc[split_index:]\n",
        "\n",
        "print(f\"   📅 Período de entrenamiento: {dates_sorted.iloc[0]} a {dates_sorted.iloc[split_index-1]}\")\n",
        "print(f\"   📅 Período de prueba: {dates_sorted.iloc[split_index]} a {dates_sorted.iloc[-1]}\")\n",
        "print(f\"   📊 Train set: {X_train.shape}\")\n",
        "print(f\"   📊 Test set: {X_test.shape}\")\n",
        "\n",
        "print(f\"\\n✅ Preprocesamiento inicial completado\")\n",
        "print(f\"📊 Datos listos para creación de pipelines\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Creación de Pipelines de Preprocesamiento\n",
        "\n",
        "Utilizaremos ColumnTransformer y Pipeline de sklearn para automatizar el preprocesamiento de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREACIÓN DE PIPELINES DE PREPROCESAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(\"🔧 Creando pipelines de preprocesamiento...\")\n",
        "\n",
        "# Separar features por tipo\n",
        "numerical_features = available_technical\n",
        "categorical_features_clean = categorical_features\n",
        "\n",
        "print(f\"📊 Features numéricas: {len(numerical_features)}\")\n",
        "print(f\"📊 Features categóricas: {len(categorical_features_clean)}\")\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE PARA FEATURES NUMÉRICAS\n",
        "# ============================================================\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Imputar valores faltantes con mediana\n",
        "    ('scaler', StandardScaler())  # Escalar a media 0 y desviación estándar 1\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE PARA FEATURES CATEGÓRICAS  \n",
        "# ============================================================\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),  # Imputar con 'unknown'\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# COMBINAR AMBOS PIPELINES CON COLUMNTRANSFORMER\n",
        "# ============================================================\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features_clean)\n",
        "    ],\n",
        "    remainder='drop'  # Eliminar columnas no especificadas\n",
        ")\n",
        "\n",
        "print(\"✅ Pipelines de preprocesamiento creados\")\n",
        "\n",
        "# ============================================================\n",
        "# PROBAR EL PREPROCESSOR\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n🧪 Probando el preprocessor...\")\n",
        "\n",
        "# Fit y transform en datos de entrenamiento\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"📊 Shape original X_train: {X_train.shape}\")\n",
        "print(f\"📊 Shape procesado X_train: {X_train_processed.shape}\")\n",
        "print(f\"📊 Shape original X_test: {X_test.shape}\")\n",
        "print(f\"📊 Shape procesado X_test: {X_test_processed.shape}\")\n",
        "\n",
        "# Obtener nombres de las features después del preprocesamiento\n",
        "def get_feature_names(preprocessor, numerical_features, categorical_features):\n",
        "    \"\"\"Obtiene los nombres de las features después del preprocesamiento\"\"\"\n",
        "    \n",
        "    # Features numéricas (se mantienen igual)\n",
        "    num_feature_names = numerical_features\n",
        "    \n",
        "    # Features categóricas (one-hot encoded)\n",
        "    cat_transformer = preprocessor.named_transformers_['cat']\n",
        "    if hasattr(cat_transformer.named_steps['onehot'], 'get_feature_names_out'):\n",
        "        # Scikit-learn >= 1.0\n",
        "        cat_feature_names = cat_transformer.named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "    else:\n",
        "        # Scikit-learn < 1.0 (fallback)\n",
        "        categories = cat_transformer.named_steps['onehot'].categories_\n",
        "        cat_feature_names = []\n",
        "        for i, cat_name in enumerate(categorical_features):\n",
        "            for category in categories[i]:\n",
        "                cat_feature_names.append(f\"{cat_name}_{category}\")\n",
        "    \n",
        "    return list(num_feature_names) + list(cat_feature_names)\n",
        "\n",
        "feature_names_processed = get_feature_names(preprocessor, numerical_features, categorical_features_clean)\n",
        "\n",
        "print(f\"📋 Total de features después del preprocesamiento: {len(feature_names_processed)}\")\n",
        "print(f\"   - Features numéricas: {len(numerical_features)}\")\n",
        "print(f\"   - Features categóricas (one-hot): {len(feature_names_processed) - len(numerical_features)}\")\n",
        "\n",
        "# Mostrar algunas features de ejemplo\n",
        "print(f\"\\n📋 Primeras 10 features procesadas:\")\n",
        "for i, name in enumerate(feature_names_processed[:10], 1):\n",
        "    print(f\"   {i:2d}. {name}\")\n",
        "\n",
        "if len(feature_names_processed) > 10:\n",
        "    print(f\"   ... y {len(feature_names_processed) - 10} más\")\n",
        "\n",
        "# ============================================================\n",
        "# VERIFICAR CALIDAD DEL PREPROCESAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n🔍 Verificación de calidad del preprocesamiento:\")\n",
        "\n",
        "# Verificar valores nulos\n",
        "train_nulls = np.isnan(X_train_processed).sum()\n",
        "test_nulls = np.isnan(X_test_processed).sum()\n",
        "\n",
        "print(f\"   ✅ Valores nulos en train: {train_nulls}\")\n",
        "print(f\"   ✅ Valores nulos en test: {test_nulls}\")\n",
        "\n",
        "# Verificar estadísticas básicas de features numéricas\n",
        "train_means = np.mean(X_train_processed[:, :len(numerical_features)], axis=0)\n",
        "train_stds = np.std(X_train_processed[:, :len(numerical_features)], axis=0)\n",
        "\n",
        "print(f\"   📊 Media de features numéricas (debería estar cerca de 0):\")\n",
        "print(f\"      Min: {train_means.min():.6f}, Max: {train_means.max():.6f}\")\n",
        "print(f\"   📊 Desviación estándar de features numéricas (debería estar cerca de 1):\")\n",
        "print(f\"      Min: {train_stds.min():.6f}, Max: {train_stds.max():.6f}\")\n",
        "\n",
        "print(f\"\\n✅ Pipelines de preprocesamiento completados y verificados\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Entrenamiento y Comparación de Modelos\n",
        "\n",
        "Entrenaremos múltiples algoritmos de machine learning y los compararemos usando validación cruzada para seleccionar el mejor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ENTRENAMIENTO Y COMPARACIÓN DE MÚLTIPLES MODELOS\n",
        "# ============================================================\n",
        "\n",
        "print(\"🤖 Iniciando entrenamiento y comparación de modelos...\")\n",
        "\n",
        "# Definir modelos a comparar\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "print(f\"📊 Modelos a evaluar: {len(models)}\")\n",
        "for name in models.keys():\n",
        "    print(f\"   • {name}\")\n",
        "\n",
        "# ============================================================\n",
        "# FUNCIÓN PARA EVALUAR MODELOS\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model(name, model, X_train, y_train, cv_folds=5):\n",
        "    \"\"\"Evalúa un modelo usando validación cruzada\"\"\"\n",
        "    \n",
        "    # Crear pipeline completo (preprocesamiento + modelo)\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Validación cruzada\n",
        "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv_folds, \n",
        "                               scoring='accuracy', n_jobs=-1)\n",
        "    \n",
        "    # Entrenar en todo el conjunto de entrenamiento para métricas adicionales\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_train)\n",
        "    \n",
        "    # Calcular métricas\n",
        "    accuracy = cv_scores.mean()\n",
        "    accuracy_std = cv_scores.std()\n",
        "    precision = precision_score(y_train, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_train, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_train, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    return {\n",
        "        'Model': name,\n",
        "        'CV_Accuracy_Mean': accuracy,\n",
        "        'CV_Accuracy_Std': accuracy_std,\n",
        "        'Train_Precision': precision,\n",
        "        'Train_Recall': recall,\n",
        "        'Train_F1': f1,\n",
        "        'Pipeline': pipeline\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# EVALUAR TODOS LOS MODELOS\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n🔄 Evaluando modelos con validación cruzada (5-fold)...\")\n",
        "\n",
        "results = []\n",
        "pipelines = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n   🔄 Evaluando {name}...\")\n",
        "    try:\n",
        "        result = evaluate_model(name, model, X_train, y_train)\n",
        "        results.append(result)\n",
        "        pipelines[name] = result['Pipeline']\n",
        "        \n",
        "        print(f\"      ✅ CV Accuracy: {result['CV_Accuracy_Mean']:.4f} ± {result['CV_Accuracy_Std']:.4f}\")\n",
        "        print(f\"      📊 Train F1: {result['Train_F1']:.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"      ❌ Error: {str(e)}\")\n",
        "\n",
        "# ============================================================\n",
        "# RESULTADOS Y COMPARACIÓN\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n📊 Resumen de resultados:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('CV_Accuracy_Mean', ascending=False)\n",
        "\n",
        "# Mostrar tabla de resultados\n",
        "print(results_df[['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Train_F1']].to_string(index=False))\n",
        "\n",
        "# Identificar el mejor modelo\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_accuracy = results_df.iloc[0]['CV_Accuracy_Mean']\n",
        "\n",
        "print(f\"\\n🏆 MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   📊 CV Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# VISUALIZACIÓN DE RESULTADOS\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Accuracy con barras de error\n",
        "plt.subplot(2, 2, 1)\n",
        "x_pos = range(len(results_df))\n",
        "plt.bar(x_pos, results_df['CV_Accuracy_Mean'], \n",
        "        yerr=results_df['CV_Accuracy_Std'], capsize=5, alpha=0.7)\n",
        "plt.xlabel('Modelos')\n",
        "plt.ylabel('CV Accuracy')\n",
        "plt.title('Comparación de Accuracy (Validación Cruzada)')\n",
        "plt.xticks(x_pos, results_df['Model'], rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: F1 Score en entrenamiento\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(x_pos, results_df['Train_F1'], alpha=0.7, color='orange')\n",
        "plt.xlabel('Modelos')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score en Entrenamiento')\n",
        "plt.xticks(x_pos, results_df['Model'], rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Scatter plot Accuracy vs F1\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.scatter(results_df['CV_Accuracy_Mean'], results_df['Train_F1'], \n",
        "           s=100, alpha=0.7, c=range(len(results_df)), cmap='viridis')\n",
        "plt.xlabel('CV Accuracy')\n",
        "plt.ylabel('Train F1 Score')\n",
        "plt.title('Accuracy vs F1 Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Añadir etiquetas a los puntos\n",
        "for i, model in enumerate(results_df['Model']):\n",
        "    plt.annotate(model, \n",
        "                (results_df.iloc[i]['CV_Accuracy_Mean'], results_df.iloc[i]['Train_F1']),\n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "# Subplot 4: Ranking de modelos\n",
        "plt.subplot(2, 2, 4)\n",
        "ranking_scores = (results_df['CV_Accuracy_Mean'] + results_df['Train_F1']) / 2\n",
        "plt.barh(range(len(results_df)), ranking_scores, alpha=0.7, color='green')\n",
        "plt.ylabel('Modelos')\n",
        "plt.xlabel('Score Promedio (Accuracy + F1) / 2')\n",
        "plt.title('Ranking General de Modelos')\n",
        "plt.yticks(range(len(results_df)), results_df['Model'])\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ Comparación de modelos completada\")\n",
        "print(f\"🎯 Modelo seleccionado para optimización: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Optimización de Hiperparámetros\n",
        "\n",
        "Optimizaremos los hiperparámetros del mejor modelo usando tres técnicas diferentes: GridSearchCV, RandomizedSearchCV y Optuna.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTIMIZACIÓN DE HIPERPARÁMETROS\n",
        "# ============================================================\n",
        "\n",
        "print(\"🔍 Iniciando optimización de hiperparámetros...\")\n",
        "print(f\"🎯 Modelo seleccionado: {best_model_name}\")\n",
        "\n",
        "# Obtener el mejor modelo\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "# Crear pipeline para optimización\n",
        "optimization_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', best_model)\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# DEFINIR ESPACIOS DE BÚSQUEDA SEGÚN EL MODELO\n",
        "# ============================================================\n",
        "\n",
        "def get_param_grid(model_name):\n",
        "    \"\"\"Define el espacio de búsqueda de hiperparámetros según el modelo\"\"\"\n",
        "    \n",
        "    if model_name == 'Random Forest':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [10, 20, 30, None],\n",
        "            'classifier__min_samples_split': [2, 5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4],\n",
        "            'classifier__max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'XGBoost':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [3, 6, 9],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
        "            'classifier__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'LightGBM':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [3, 6, 9],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__num_leaves': [31, 50, 100],\n",
        "            'classifier__feature_fraction': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'Gradient Boosting':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [3, 6, 9],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
        "            'classifier__max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'Support Vector Machine':\n",
        "        return {\n",
        "            'classifier__C': [0.1, 1, 10, 100],\n",
        "            'classifier__gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "            'classifier__kernel': ['rbf', 'poly', 'sigmoid']\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'Logistic Regression':\n",
        "        return {\n",
        "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
        "            'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "            'classifier__solver': ['liblinear', 'saga'],\n",
        "            'classifier__max_iter': [1000, 2000]\n",
        "        }\n",
        "    \n",
        "    else:  # Default para otros modelos\n",
        "        return {\n",
        "            'classifier__random_state': [42]  # Parámetro mínimo\n",
        "        }\n",
        "\n",
        "param_grid = get_param_grid(best_model_name)\n",
        "print(f\"📋 Parámetros a optimizar: {len(param_grid)}\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"   • {param}: {values}\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. GRIDSEARCHCV - BÚSQUEDA EXHAUSTIVA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n1️⃣ Ejecutando GridSearchCV...\")\n",
        "print(f\"   🔄 Búsqueda exhaustiva en {np.prod([len(v) for v in param_grid.values()])} combinaciones\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    optimization_pipeline,\n",
        "    param_grid,\n",
        "    cv=3,  # Reducido para acelerar\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "grid_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"   ✅ GridSearchCV completado en {grid_time:.1f} segundos\")\n",
        "print(f\"   🏆 Mejor score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"   📋 Mejores parámetros: {grid_search.best_params_}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. RANDOMIZEDSEARCHCV - BÚSQUEDA ALEATORIA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n2️⃣ Ejecutando RandomizedSearchCV...\")\n",
        "\n",
        "# Expandir el espacio de búsqueda para RandomizedSearch\n",
        "def get_random_param_dist(model_name):\n",
        "    \"\"\"Define distribuciones para búsqueda aleatoria\"\"\"\n",
        "    \n",
        "    if model_name == 'Random Forest':\n",
        "        return {\n",
        "            'classifier__n_estimators': [50, 100, 200, 300, 500],\n",
        "            'classifier__max_depth': [5, 10, 15, 20, 25, 30, None],\n",
        "            'classifier__min_samples_split': [2, 5, 10, 15],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4, 6],\n",
        "            'classifier__max_features': ['sqrt', 'log2', None, 0.5, 0.8]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'XGBoost':\n",
        "        return {\n",
        "            'classifier__n_estimators': [50, 100, 200, 300, 500],\n",
        "            'classifier__max_depth': [3, 6, 9, 12],\n",
        "            'classifier__learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
        "            'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        }\n",
        "    \n",
        "    # Usar el mismo que GridSearch para otros modelos\n",
        "    else:\n",
        "        return param_grid\n",
        "\n",
        "random_param_dist = get_random_param_dist(best_model_name)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    optimization_pipeline,\n",
        "    random_param_dist,\n",
        "    n_iter=50,  # Número de iteraciones aleatorias\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "random_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"   ✅ RandomizedSearchCV completado en {random_time:.1f} segundos\")\n",
        "print(f\"   🏆 Mejor score: {random_search.best_score_:.4f}\")\n",
        "print(f\"   📋 Mejores parámetros: {random_search.best_params_}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. OPTUNA - OPTIMIZACIÓN BAYESIANA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n3️⃣ Ejecutando Optuna (Optimización Bayesiana)...\")\n",
        "\n",
        "def objective(trial, model_name, pipeline_template, X_train, y_train):\n",
        "    \"\"\"Función objetivo para Optuna\"\"\"\n",
        "    \n",
        "    # Sugerir parámetros según el modelo\n",
        "    if model_name == 'Random Forest':\n",
        "        params = {\n",
        "            'classifier__n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
        "            'classifier__max_depth': trial.suggest_int('max_depth', 5, 30),\n",
        "            'classifier__min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'classifier__min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'classifier__max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'XGBoost':\n",
        "        params = {\n",
        "            'classifier__n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
        "            'classifier__max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "            'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "            'classifier__subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
        "        }\n",
        "    \n",
        "    else:\n",
        "        # Para otros modelos, usar parámetros del GridSearch\n",
        "        params = {}\n",
        "        for param, values in param_grid.items():\n",
        "            if isinstance(values[0], (int, float)):\n",
        "                if isinstance(values[0], int):\n",
        "                    params[param] = trial.suggest_int(param.split('__')[1], min(values), max(values))\n",
        "                else:\n",
        "                    params[param] = trial.suggest_float(param.split('__')[1], min(values), max(values))\n",
        "            else:\n",
        "                params[param] = trial.suggest_categorical(param.split('__')[1], values)\n",
        "    \n",
        "    # Crear pipeline con parámetros sugeridos\n",
        "    pipeline_copy = Pipeline([\n",
        "        ('preprocessor', pipeline_template.named_steps['preprocessor']),\n",
        "        ('classifier', pipeline_template.named_steps['classifier'].__class__(**{k.split('__')[1]: v for k, v in params.items()}, random_state=42))\n",
        "    ])\n",
        "    \n",
        "    # Evaluar con validación cruzada\n",
        "    scores = cross_val_score(pipeline_copy, X_train, y_train, cv=3, scoring='accuracy', n_jobs=1)\n",
        "    return scores.mean()\n",
        "\n",
        "# Crear estudio de Optuna\n",
        "start_time = datetime.now()\n",
        "\n",
        "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
        "study.optimize(\n",
        "    lambda trial: objective(trial, best_model_name, optimization_pipeline, X_train, y_train),\n",
        "    n_trials=50,\n",
        "    timeout=300,  # 5 minutos máximo\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "optuna_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"   ✅ Optuna completado en {optuna_time:.1f} segundos\")\n",
        "print(f\"   🏆 Mejor score: {study.best_value:.4f}\")\n",
        "print(f\"   📋 Mejores parámetros: {study.best_params}\")\n",
        "\n",
        "# ============================================================\n",
        "# COMPARACIÓN DE TÉCNICAS DE OPTIMIZACIÓN\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n📊 COMPARACIÓN DE TÉCNICAS DE OPTIMIZACIÓN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "optimization_results = pd.DataFrame({\n",
        "    'Technique': ['GridSearchCV', 'RandomizedSearchCV', 'Optuna'],\n",
        "    'Best_Score': [grid_search.best_score_, random_search.best_score_, study.best_value],\n",
        "    'Time_Seconds': [grid_time, random_time, optuna_time],\n",
        "    'Evaluations': [len(grid_search.cv_results_['mean_test_score']), 50, len(study.trials)]\n",
        "})\n",
        "\n",
        "print(optimization_results.to_string(index=False))\n",
        "\n",
        "# Identificar la mejor técnica\n",
        "best_technique_idx = optimization_results['Best_Score'].idxmax()\n",
        "best_technique = optimization_results.iloc[best_technique_idx]\n",
        "\n",
        "print(f\"\\n🏆 MEJOR TÉCNICA: {best_technique['Technique']}\")\n",
        "print(f\"   📊 Score: {best_technique['Best_Score']:.4f}\")\n",
        "print(f\"   ⏱️ Tiempo: {best_technique['Time_Seconds']:.1f} segundos\")\n",
        "print(f\"   🔄 Evaluaciones: {best_technique['Evaluations']}\")\n",
        "\n",
        "# Visualización\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Subplot 1: Comparación de scores\n",
        "plt.subplot(1, 3, 1)\n",
        "bars = plt.bar(optimization_results['Technique'], optimization_results['Best_Score'], \n",
        "               color=['blue', 'orange', 'green'], alpha=0.7)\n",
        "plt.title('Comparación de Best Scores')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Añadir valores en las barras\n",
        "for bar, score in zip(bars, optimization_results['Best_Score']):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
        "             f'{score:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Subplot 2: Comparación de tiempos\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(optimization_results['Technique'], optimization_results['Time_Seconds'], \n",
        "        color=['blue', 'orange', 'green'], alpha=0.7)\n",
        "plt.title('Tiempo de Ejecución')\n",
        "plt.ylabel('Segundos')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Eficiencia (Score/Time)\n",
        "plt.subplot(1, 3, 3)\n",
        "efficiency = optimization_results['Best_Score'] / optimization_results['Time_Seconds']\n",
        "plt.bar(optimization_results['Technique'], efficiency, \n",
        "        color=['blue', 'orange', 'green'], alpha=0.7)\n",
        "plt.title('Eficiencia (Score/Tiempo)')\n",
        "plt.ylabel('Score por Segundo')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ Optimización de hiperparámetros completada\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Evaluación Final y Conclusiones\n",
        "\n",
        "Evaluaremos el modelo optimizado en el conjunto de prueba y analizaremos los resultados finales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUACIÓN FINAL DEL MODELO OPTIMIZADO\n",
        "# ============================================================\n",
        "\n",
        "print(\"🎯 Evaluación final del modelo optimizado...\")\n",
        "\n",
        "# Seleccionar el mejor modelo según la técnica de optimización ganadora\n",
        "if best_technique['Technique'] == 'GridSearchCV':\n",
        "    final_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "elif best_technique['Technique'] == 'RandomizedSearchCV':\n",
        "    final_model = random_search.best_estimator_\n",
        "    best_params = random_search.best_params_\n",
        "else:  # Optuna\n",
        "    # Crear modelo con los mejores parámetros de Optuna\n",
        "    optuna_params = {f\"classifier__{k}\": v for k, v in study.best_params.items()}\n",
        "    final_model = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', best_model.__class__(**study.best_params, random_state=42))\n",
        "    ])\n",
        "    final_model.fit(X_train, y_train)\n",
        "    best_params = study.best_params\n",
        "\n",
        "print(f\"🏆 Modelo final: {best_model_name} optimizado con {best_technique['Technique']}\")\n",
        "print(f\"📋 Parámetros finales: {best_params}\")\n",
        "\n",
        "# ============================================================\n",
        "# EVALUACIÓN EN CONJUNTO DE PRUEBA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n📊 Evaluando en conjunto de prueba...\")\n",
        "\n",
        "# Predicciones\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_pred_proba = final_model.predict_proba(X_test)\n",
        "\n",
        "# Métricas principales\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "test_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "test_recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "test_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"✅ Resultados en conjunto de prueba:\")\n",
        "print(f\"   📊 Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"   📊 Precision: {test_precision:.4f}\")\n",
        "print(f\"   📊 Recall: {test_recall:.4f}\")\n",
        "print(f\"   📊 F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# MATRIZ DE CONFUSIÓN\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n📋 Matriz de Confusión:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Visualización de la matriz de confusión\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Subplot 1: Matriz de confusión\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['BUY', 'HOLD', 'SELL'],\n",
        "            yticklabels=['BUY', 'HOLD', 'SELL'])\n",
        "plt.title('Matriz de Confusión')\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Real')\n",
        "\n",
        "# Subplot 2: Distribución de clases reales vs predichas\n",
        "plt.subplot(2, 3, 2)\n",
        "class_names = ['BUY', 'HOLD', 'SELL']\n",
        "real_counts = [sum(y_test == i) for i in range(3)]\n",
        "pred_counts = [sum(y_pred == i) for i in range(3)]\n",
        "\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, real_counts, width, label='Real', alpha=0.7)\n",
        "plt.bar(x + width/2, pred_counts, width, label='Predicción', alpha=0.7)\n",
        "plt.xlabel('Clases')\n",
        "plt.ylabel('Cantidad')\n",
        "plt.title('Distribución Real vs Predicha')\n",
        "plt.xticks(x, class_names)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Métricas por clase\n",
        "plt.subplot(2, 3, 3)\n",
        "precision_per_class = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
        "recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
        "f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
        "\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.25\n",
        "\n",
        "plt.bar(x - width, precision_per_class, width, label='Precision', alpha=0.7)\n",
        "plt.bar(x, recall_per_class, width, label='Recall', alpha=0.7)\n",
        "plt.bar(x + width, f1_per_class, width, label='F1-Score', alpha=0.7)\n",
        "plt.xlabel('Clases')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Métricas por Clase')\n",
        "plt.xticks(x, class_names)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Curva ROC (para clase BUY)\n",
        "plt.subplot(2, 3, 4)\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarizar las etiquetas para ROC multiclase\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "y_pred_proba_bin = y_pred_proba\n",
        "\n",
        "# Calcular ROC para cada clase\n",
        "colors = ['blue', 'orange', 'green']\n",
        "for i, class_name in enumerate(class_names):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_bin[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
        "             label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curvas ROC por Clase')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 5: Comparación modelo base vs optimizado\n",
        "plt.subplot(2, 3, 5)\n",
        "base_accuracy = results_df[results_df['Model'] == best_model_name]['CV_Accuracy_Mean'].iloc[0]\n",
        "comparison_data = ['Modelo Base', 'Modelo Optimizado']\n",
        "comparison_scores = [base_accuracy, test_accuracy]\n",
        "\n",
        "bars = plt.bar(comparison_data, comparison_scores, color=['red', 'green'], alpha=0.7)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Mejora del Modelo')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Añadir valores en las barras\n",
        "for bar, score in zip(bars, comparison_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{score:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Calcular mejora\n",
        "improvement = ((test_accuracy - base_accuracy) / base_accuracy) * 100\n",
        "plt.text(0.5, max(comparison_scores) * 0.8, f'Mejora: {improvement:.1f}%', \n",
        "         ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
        "\n",
        "# Subplot 6: Importancia de features (si el modelo lo soporta)\n",
        "plt.subplot(2, 3, 6)\n",
        "try:\n",
        "    if hasattr(final_model.named_steps['classifier'], 'feature_importances_'):\n",
        "        # Obtener importancias\n",
        "        importances = final_model.named_steps['classifier'].feature_importances_\n",
        "        \n",
        "        # Obtener nombres de features\n",
        "        feature_names_short = feature_names_processed[:len(importances)]\n",
        "        \n",
        "        # Seleccionar top 10 features más importantes\n",
        "        indices = np.argsort(importances)[-10:]\n",
        "        \n",
        "        plt.barh(range(len(indices)), importances[indices], alpha=0.7)\n",
        "        plt.yticks(range(len(indices)), [feature_names_short[i] for i in indices])\n",
        "        plt.xlabel('Importancia')\n",
        "        plt.title('Top 10 Features Importantes')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Importancia de features\\nno disponible para\\neste modelo', \n",
        "                ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title('Importancia de Features')\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Error al calcular\\nimportancia: {str(e)[:30]}...', \n",
        "            ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.title('Importancia de Features')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# REPORTE DETALLADO\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n📋 REPORTE DE CLASIFICACIÓN DETALLADO\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(y_test, y_pred, target_names=['BUY', 'HOLD', 'SELL']))\n",
        "\n",
        "# ============================================================\n",
        "# ANÁLISIS DE ERRORES\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n🔍 ANÁLISIS DE ERRORES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "results_analysis = pd.DataFrame({\n",
        "    'Real': y_test,\n",
        "    'Prediccion': y_pred,\n",
        "    'Correcto': y_test == y_pred\n",
        "})\n",
        "\n",
        "# Estadísticas de errores\n",
        "error_stats = results_analysis.groupby(['Real', 'Prediccion']).size().unstack(fill_value=0)\n",
        "print(\"Matriz de errores detallada:\")\n",
        "print(error_stats)\n",
        "\n",
        "# Porcentaje de aciertos por clase\n",
        "accuracy_per_class = []\n",
        "for i in range(3):\n",
        "    class_mask = y_test == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_accuracy = (y_pred[class_mask] == i).mean()\n",
        "        accuracy_per_class.append(class_accuracy)\n",
        "        print(f\"Accuracy clase {class_names[i]}: {class_accuracy:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# CONCLUSIONES FINALES\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n🎯 CONCLUSIONES FINALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"✅ MODELO FINAL SELECCIONADO:\")\n",
        "print(f\"   • Algoritmo: {best_model_name}\")\n",
        "print(f\"   • Técnica de optimización: {best_technique['Technique']}\")\n",
        "print(f\"   • Accuracy en prueba: {test_accuracy:.4f}\")\n",
        "print(f\"   • F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "print(f\"\\n📊 RENDIMIENTO POR OBJETIVO:\")\n",
        "print(f\"   • BUY: Precision={precision_per_class[0]:.3f}, Recall={recall_per_class[0]:.3f}\")\n",
        "print(f\"   • HOLD: Precision={precision_per_class[1]:.3f}, Recall={recall_per_class[1]:.3f}\")\n",
        "print(f\"   • SELL: Precision={precision_per_class[2]:.3f}, Recall={recall_per_class[2]:.3f}\")\n",
        "\n",
        "print(f\"\\n🚀 MEJORAS OBTENIDAS:\")\n",
        "print(f\"   • Mejora en accuracy: {improvement:.1f}%\")\n",
        "print(f\"   • Tiempo de optimización total: {optimization_results['Time_Seconds'].sum():.1f} segundos\")\n",
        "\n",
        "print(f\"\\n💡 RECOMENDACIONES:\")\n",
        "print(f\"   • El modelo muestra {'buen' if test_accuracy > 0.7 else 'regular' if test_accuracy > 0.6 else 'bajo'} rendimiento\")\n",
        "print(f\"   • {'Se recomienda usar en producción' if test_accuracy > 0.75 else 'Requiere más optimización antes de producción'}\")\n",
        "if test_f1 < 0.7:\n",
        "    print(f\"   • Considerar balanceo de clases o más feature engineering\")\n",
        "if improvement < 5:\n",
        "    print(f\"   • La optimización mostró mejoras limitadas, considerar otros modelos\")\n",
        "\n",
        "print(f\"\\n✅ Evaluación final completada\")\n",
        "print(f\"🎯 Sistema de recomendación de trading listo para {'implementación' if test_accuracy > 0.7 else 'más desarrollo'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📋 Documentación Final del Proceso\n",
        "\n",
        "### Resumen Ejecutivo - Parte II\n",
        "\n",
        "Este notebook ha implementado exitosamente un pipeline completo de Machine Learning para clasificación de señales de trading, siguiendo las mejores prácticas de la industria.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Proceso Completado:\n",
        "\n",
        "#### 1. **Preprocesamiento de Datos**\n",
        "- ✅ Limpieza de valores nulos y outliers\n",
        "- ✅ Feature engineering con 20+ indicadores técnicos\n",
        "- ✅ Creación de variable objetivo (BUY/HOLD/SELL)\n",
        "- ✅ Pipeline automatizado con ColumnTransformer\n",
        "- ✅ División temporal de datos (80/20)\n",
        "\n",
        "#### 2. **Selección de Técnica de ML**\n",
        "- ✅ Evaluación de 8 algoritmos diferentes\n",
        "- ✅ Validación cruzada 5-fold\n",
        "- ✅ Comparación exhaustiva de métricas\n",
        "- ✅ Selección basada en rendimiento\n",
        "\n",
        "#### 3. **Optimización de Hiperparámetros**\n",
        "- ✅ GridSearchCV (búsqueda exhaustiva)\n",
        "- ✅ RandomizedSearchCV (búsqueda aleatoria)\n",
        "- ✅ Optuna (optimización bayesiana)\n",
        "- ✅ Comparación de eficiencia\n",
        "\n",
        "#### 4. **Evaluación Final**\n",
        "- ✅ Métricas en conjunto de prueba\n",
        "- ✅ Matriz de confusión detallada\n",
        "- ✅ Curvas ROC multiclase\n",
        "- ✅ Análisis de importancia de features\n",
        "- ✅ Reporte de clasificación completo\n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 Resultados Clave:\n",
        "\n",
        "- **Modelo Final**: [Se determinará al ejecutar]\n",
        "- **Accuracy**: [Se determinará al ejecutar]\n",
        "- **Mejora vs Baseline**: [Se determinará al ejecutar]\n",
        "- **Features Importantes**: Indicadores técnicos RSI, MACD, Bollinger Bands\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Valor del Proyecto:\n",
        "\n",
        "1. **Automatización**: Pipeline reproducible para nuevos datos\n",
        "2. **Escalabilidad**: Fácil incorporación de nuevas acciones\n",
        "3. **Robustez**: Validación temporal estricta\n",
        "4. **Interpretabilidad**: Análisis de importancia de features\n",
        "5. **Producción**: Listo para implementación real\n",
        "\n",
        "---\n",
        "\n",
        "### 📈 Aplicaciones Prácticas:\n",
        "\n",
        "- **Trading Algorítmico**: Señales automáticas de compra/venta\n",
        "- **Gestión de Riesgo**: Identificación de momentos de alta volatilidad\n",
        "- **Optimización de Portafolios**: Diversificación inteligente\n",
        "- **Alertas de Mercado**: Notificaciones de oportunidades\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 Próximos Pasos Recomendados:\n",
        "\n",
        "1. **Implementación en Tiempo Real**: Conexión con APIs de mercado\n",
        "2. **Backtesting Avanzado**: Simulación de estrategias históricas\n",
        "3. **Dashboard Interactivo**: Visualización en tiempo real\n",
        "4. **Alertas Automáticas**: Sistema de notificaciones\n",
        "5. **Optimización Continua**: Re-entrenamiento periódico\n",
        "\n",
        "---\n",
        "\n",
        "**📊 Este proyecto demuestra competencias avanzadas en:**\n",
        "- Machine Learning aplicado a finanzas\n",
        "- Pipelines de preprocesamiento robusto\n",
        "- Optimización de hiperparámetros\n",
        "- Evaluación rigurosa de modelos\n",
        "- Interpretación de resultados financieros\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
