{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Proyecto 2 - Parte II: Preprocesamiento y Optimizaci√≥n\n",
        "\n",
        "## Sistema Inteligente de Recomendaci√≥n de Inversiones\n",
        "### Clasificaci√≥n de Se√±ales de Trading (Buy/Hold/Sell)\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivos de la Parte II\n",
        "\n",
        "1. **Preprocesamiento de Datos:**\n",
        "   - Limpieza de datos (valores nulos, outliers)\n",
        "   - Transformaci√≥n de columnas con ColumnTransformer\n",
        "   - Creaci√≥n de indicadores t√©cnicos (features)\n",
        "   - Escalado y normalizaci√≥n\n",
        "   - Pipelines automatizados\n",
        "\n",
        "2. **Selecci√≥n de T√©cnica de ML:**\n",
        "   - Entrenamiento de m√∫ltiples modelos\n",
        "   - Evaluaci√≥n con validaci√≥n cruzada\n",
        "   - Comparaci√≥n de rendimiento\n",
        "\n",
        "3. **Optimizaci√≥n de Hiperpar√°metros:**\n",
        "   - GridSearchCV\n",
        "   - RandomizedSearchCV\n",
        "   - Optuna (optimizaci√≥n bayesiana)\n",
        "\n",
        "4. **Evaluaci√≥n Final:**\n",
        "   - M√©tricas de rendimiento\n",
        "   - An√°lisis de resultados\n",
        "   - Recomendaciones\n",
        "\n",
        "---\n",
        "\n",
        "### Problema Seleccionado: Clasificaci√≥n de Se√±ales de Trading\n",
        "\n",
        "**Target:** Predecir se√±ales de inversi√≥n basadas en indicadores t√©cnicos\n",
        "- **BUY (0):** Momento favorable para comprar \n",
        "- **HOLD (1):** Mantener posici√≥n actual\n",
        "- **SELL (2):** Momento favorable para vender\n",
        "\n",
        "**Justificaci√≥n:** Este problema es altamente pr√°ctico para inversores y combina an√°lisis t√©cnico con machine learning para tomar decisiones informadas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Modelos de ML\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# M√©tricas\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve\n",
        "\n",
        "# Optimizaci√≥n avanzada\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"‚úÖ Optuna disponible para optimizaci√≥n bayesiana\")\n",
        "except ImportError:\n",
        "    OPTUNA_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è Optuna no disponible. Instalando...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "    print(\"‚úÖ Optuna instalado exitosamente\")\n",
        "\n",
        "# Configuraci√≥n\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"üöÄ Librer√≠as importadas exitosamente\")\n",
        "print(\"üìä Iniciando Parte II: Preprocesamiento y Optimizaci√≥n\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Carga y An√°lisis Inicial de Datos\n",
        "\n",
        "Cargaremos el dataset financiero combinado y realizaremos un an√°lisis inicial para entender la estructura de los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset combinado\n",
        "print(\"üìà Cargando dataset financiero combinado...\")\n",
        "df = pd.read_csv('selected_dataset/financial_data_combined.csv')\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "print(f\"‚úÖ Dataset cargado exitosamente\")\n",
        "print(f\"üìä Forma del dataset: {df.shape}\")\n",
        "print(f\"üóìÔ∏è Per√≠odo: {df['Date'].min()} a {df['Date'].max()}\")\n",
        "print(f\"üè¢ Acciones: {', '.join(df['Stock'].unique())}\")\n",
        "print(f\"üè≠ Sectores: {', '.join(df['Sector'].unique())}\")\n",
        "\n",
        "# Informaci√≥n b√°sica del dataset\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INFORMACI√ìN DEL DATASET\")\n",
        "print(\"=\"*50)\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ESTAD√çSTICAS DESCRIPTIVAS\")\n",
        "print(\"=\"*50)\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"VALORES NULOS\")\n",
        "print(\"=\"*50)\n",
        "null_counts = df.isnull().sum()\n",
        "null_percentages = (null_counts / len(df)) * 100\n",
        "null_summary = pd.DataFrame({\n",
        "    'Valores Nulos': null_counts,\n",
        "    'Porcentaje': null_percentages\n",
        "})\n",
        "print(null_summary[null_summary['Valores Nulos'] > 0])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRIMERAS 5 FILAS\")\n",
        "print(\"=\"*50)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Feature Engineering - Creaci√≥n de Indicadores T√©cnicos\n",
        "\n",
        "Crearemos indicadores t√©cnicos que servir√°n como features para nuestro modelo de clasificaci√≥n de se√±ales de trading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Crea indicadores t√©cnicos para an√°lisis de trading\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    print(\"üîß Creando indicadores t√©cnicos...\")\n",
        "    \n",
        "    # Agrupar por acci√≥n para calcular indicadores por separado\n",
        "    dfs_processed = []\n",
        "    \n",
        "    for stock in df['Stock'].unique():\n",
        "        stock_df = df[df['Stock'] == stock].copy()\n",
        "        stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
        "        \n",
        "        # ========== INDICADORES DE PRECIO ==========\n",
        "        \n",
        "        # Retornos\n",
        "        stock_df['Daily_Return'] = stock_df['Adjusted Close'].pct_change()\n",
        "        stock_df['Price_Change'] = stock_df['Adjusted Close'].diff()\n",
        "        \n",
        "        # Promedios m√≥viles\n",
        "        stock_df['MA_5'] = stock_df['Adjusted Close'].rolling(window=5).mean()\n",
        "        stock_df['MA_10'] = stock_df['Adjusted Close'].rolling(window=10).mean()\n",
        "        stock_df['MA_20'] = stock_df['Adjusted Close'].rolling(window=20).mean()\n",
        "        stock_df['MA_50'] = stock_df['Adjusted Close'].rolling(window=50, min_periods=30).mean()\n",
        "        \n",
        "        # Exponential Moving Averages\n",
        "        stock_df['EMA_12'] = stock_df['Adjusted Close'].ewm(span=12).mean()\n",
        "        stock_df['EMA_26'] = stock_df['Adjusted Close'].ewm(span=26).mean()\n",
        "        \n",
        "        # ========== RSI (Relative Strength Index) ==========\n",
        "        delta = stock_df['Adjusted Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        stock_df['RSI'] = 100 - (100 / (1 + rs))\n",
        "        \n",
        "        # ========== MACD ==========\n",
        "        stock_df['MACD'] = stock_df['EMA_12'] - stock_df['EMA_26']\n",
        "        stock_df['MACD_Signal'] = stock_df['MACD'].ewm(span=9).mean()\n",
        "        stock_df['MACD_Histogram'] = stock_df['MACD'] - stock_df['MACD_Signal']\n",
        "        \n",
        "        # ========== BOLLINGER BANDS ==========\n",
        "        stock_df['BB_Middle'] = stock_df['MA_20']\n",
        "        bb_std = stock_df['Adjusted Close'].rolling(window=20).std()\n",
        "        stock_df['BB_Upper'] = stock_df['BB_Middle'] + (bb_std * 2)\n",
        "        stock_df['BB_Lower'] = stock_df['BB_Middle'] - (bb_std * 2)\n",
        "        stock_df['BB_Width'] = stock_df['BB_Upper'] - stock_df['BB_Lower']\n",
        "        stock_df['BB_Position'] = (stock_df['Adjusted Close'] - stock_df['BB_Lower']) / stock_df['BB_Width']\n",
        "        \n",
        "        # ========== STOCHASTIC OSCILLATOR ==========\n",
        "        high_14 = stock_df['High'].rolling(window=14).max()\n",
        "        low_14 = stock_df['Low'].rolling(window=14).min()\n",
        "        stock_df['Stoch_K'] = 100 * (stock_df['Close'] - low_14) / (high_14 - low_14)\n",
        "        stock_df['Stoch_D'] = stock_df['Stoch_K'].rolling(window=3).mean()\n",
        "        \n",
        "        # ========== WILLIAMS %R ==========\n",
        "        stock_df['Williams_R'] = -100 * (high_14 - stock_df['Close']) / (high_14 - low_14)\n",
        "        \n",
        "        # ========== INDICADORES DE VOLUMEN ==========\n",
        "        stock_df['Volume_MA'] = stock_df['Volume'].rolling(window=20).mean()\n",
        "        stock_df['Volume_Ratio'] = stock_df['Volume'] / stock_df['Volume_MA']\n",
        "        \n",
        "        # Money Flow Index (MFI)\n",
        "        typical_price = (stock_df['High'] + stock_df['Low'] + stock_df['Close']) / 3\n",
        "        money_flow = typical_price * stock_df['Volume']\n",
        "        \n",
        "        positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0).rolling(window=14).sum()\n",
        "        negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0).rolling(window=14).sum()\n",
        "        \n",
        "        mfi_ratio = positive_flow / negative_flow\n",
        "        stock_df['MFI'] = 100 - (100 / (1 + mfi_ratio))\n",
        "        \n",
        "        # ========== INDICADORES DE VOLATILIDAD ==========\n",
        "        stock_df['Volatility'] = stock_df['Daily_Return'].rolling(window=20).std()\n",
        "        stock_df['ATR'] = ((stock_df['High'] - stock_df['Low']).rolling(window=14).mean())\n",
        "        \n",
        "        # ========== RATIOS DE PRECIO ==========\n",
        "        stock_df['Price_to_MA20'] = stock_df['Adjusted Close'] / stock_df['MA_20']\n",
        "        stock_df['MA5_to_MA20'] = stock_df['MA_5'] / stock_df['MA_20']\n",
        "        stock_df['High_Low_Ratio'] = stock_df['High'] / stock_df['Low']\n",
        "        \n",
        "        dfs_processed.append(stock_df)\n",
        "        print(f\"   ‚úÖ {stock}: {len([col for col in stock_df.columns if col not in df.columns])} nuevos indicadores\")\n",
        "    \n",
        "    # Combinar todos los dataframes procesados\n",
        "    df_combined = pd.concat(dfs_processed, ignore_index=True)\n",
        "    \n",
        "    print(f\"üéØ Feature Engineering completado\")\n",
        "    print(f\"üìä Nuevas columnas creadas: {len(df_combined.columns) - len(df.columns)}\")\n",
        "    print(f\"üìà Dataset final: {df_combined.shape}\")\n",
        "    \n",
        "    return df_combined\n",
        "\n",
        "# Aplicar feature engineering\n",
        "df_features = create_technical_indicators(df)\n",
        "\n",
        "# Mostrar las nuevas columnas creadas\n",
        "new_columns = [col for col in df_features.columns if col not in df.columns]\n",
        "print(f\"\\nüìã Indicadores t√©cnicos creados ({len(new_columns)}):\")\n",
        "for i, col in enumerate(new_columns, 1):\n",
        "    print(f\"   {i:2d}. {col}\")\n",
        "\n",
        "# Verificar valores nulos despu√©s del feature engineering\n",
        "print(f\"\\n‚ö†Ô∏è Valores nulos despu√©s del feature engineering:\")\n",
        "null_summary_new = df_features.isnull().sum()\n",
        "print(null_summary_new[null_summary_new > 0].head(10))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Creaci√≥n de Variable Objetivo (Target)\n",
        "\n",
        "Crearemos las etiquetas de trading basadas en una combinaci√≥n de indicadores t√©cnicos y retornos futuros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_trading_signals(df, forward_days=5, rsi_buy=30, rsi_sell=70):\n",
        "    \"\"\"\n",
        "    Crea se√±ales de trading basadas en indicadores t√©cnicos y retornos futuros\n",
        "    \n",
        "    Par√°metros:\n",
        "    - forward_days: d√≠as hacia adelante para calcular retornos futuros\n",
        "    - rsi_buy: nivel RSI para se√±al de compra\n",
        "    - rsi_sell: nivel RSI para se√±al de venta\n",
        "    \n",
        "    Se√±ales:\n",
        "    - 0: BUY - Condiciones favorables para comprar\n",
        "    - 1: HOLD - Mantener posici√≥n actual  \n",
        "    - 2: SELL - Condiciones favorables para vender\n",
        "    \"\"\"\n",
        "    \n",
        "    df = df.copy()\n",
        "    print(f\"üéØ Creando se√±ales de trading...\")\n",
        "    print(f\"   üìä RSI Buy threshold: {rsi_buy}\")\n",
        "    print(f\"   üìä RSI Sell threshold: {rsi_sell}\")\n",
        "    print(f\"   üìÖ Forward days: {forward_days}\")\n",
        "    \n",
        "    # Procesar cada acci√≥n por separado\n",
        "    dfs_with_signals = []\n",
        "    \n",
        "    for stock in df['Stock'].unique():\n",
        "        stock_df = df[df['Stock'] == stock].copy()\n",
        "        stock_df = stock_df.sort_values('Date').reset_index(drop=True)\n",
        "        \n",
        "        # Calcular retorno futuro\n",
        "        stock_df['Future_Return'] = stock_df['Adjusted Close'].pct_change(periods=forward_days).shift(-forward_days)\n",
        "        \n",
        "        # Inicializar se√±ales como HOLD (1)\n",
        "        stock_df['Trading_Signal'] = 1\n",
        "        \n",
        "        # Condiciones para BUY (0)\n",
        "        buy_conditions = (\n",
        "            (stock_df['RSI'] < rsi_buy) &  # RSI oversold\n",
        "            (stock_df['BB_Position'] < 0.2) &  # Precio cerca del l√≠mite inferior de Bollinger\n",
        "            (stock_df['MACD'] > stock_df['MACD_Signal']) &  # MACD bullish\n",
        "            (stock_df['Stoch_K'] < 20) &  # Stochastic oversold\n",
        "            (stock_df['Future_Return'] > 0.02)  # Retorno futuro positivo > 2%\n",
        "        )\n",
        "        \n",
        "        # Condiciones para SELL (2)\n",
        "        sell_conditions = (\n",
        "            (stock_df['RSI'] > rsi_sell) &  # RSI overbought\n",
        "            (stock_df['BB_Position'] > 0.8) &  # Precio cerca del l√≠mite superior de Bollinger\n",
        "            (stock_df['MACD'] < stock_df['MACD_Signal']) &  # MACD bearish\n",
        "            (stock_df['Stoch_K'] > 80) &  # Stochastic overbought\n",
        "            (stock_df['Future_Return'] < -0.02)  # Retorno futuro negativo > -2%\n",
        "        )\n",
        "        \n",
        "        # Aplicar las condiciones\n",
        "        stock_df.loc[buy_conditions, 'Trading_Signal'] = 0  # BUY\n",
        "        stock_df.loc[sell_conditions, 'Trading_Signal'] = 2  # SELL\n",
        "        \n",
        "        dfs_with_signals.append(stock_df)\n",
        "        \n",
        "        # Estad√≠sticas por acci√≥n\n",
        "        signal_counts = stock_df['Trading_Signal'].value_counts().sort_index()\n",
        "        print(f\"   üìà {stock}: BUY={signal_counts.get(0, 0)}, HOLD={signal_counts.get(1, 0)}, SELL={signal_counts.get(2, 0)}\")\n",
        "    \n",
        "    # Combinar todas las acciones\n",
        "    df_with_signals = pd.concat(dfs_with_signals, ignore_index=True)\n",
        "    \n",
        "    # Estad√≠sticas generales\n",
        "    print(f\"\\nüìä Distribuci√≥n de se√±ales general:\")\n",
        "    signal_distribution = df_with_signals['Trading_Signal'].value_counts().sort_index()\n",
        "    total_signals = len(df_with_signals.dropna(subset=['Trading_Signal']))\n",
        "    \n",
        "    for signal, count in signal_distribution.items():\n",
        "        signal_name = ['BUY', 'HOLD', 'SELL'][signal]\n",
        "        percentage = (count / total_signals) * 100\n",
        "        print(f\"   {signal_name} ({signal}): {count:,} ({percentage:.1f}%)\")\n",
        "    \n",
        "    return df_with_signals\n",
        "\n",
        "# Crear se√±ales de trading\n",
        "df_with_signals = create_trading_signals(df_features)\n",
        "\n",
        "# Visualizar la distribuci√≥n de se√±ales\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Subplot 1: Distribuci√≥n general\n",
        "plt.subplot(1, 2, 1)\n",
        "signal_counts = df_with_signals['Trading_Signal'].value_counts().sort_index()\n",
        "labels = ['BUY', 'HOLD', 'SELL']\n",
        "colors = ['green', 'orange', 'red']\n",
        "plt.pie(signal_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribuci√≥n de Se√±ales de Trading')\n",
        "\n",
        "# Subplot 2: Distribuci√≥n por acci√≥n\n",
        "plt.subplot(1, 2, 2)\n",
        "signal_by_stock = df_with_signals.groupby(['Stock', 'Trading_Signal']).size().unstack(fill_value=0)\n",
        "signal_by_stock.plot(kind='bar', color=colors, alpha=0.7)\n",
        "plt.title('Se√±ales por Acci√≥n')\n",
        "plt.xlabel('Acci√≥n')\n",
        "plt.ylabel('Cantidad de Se√±ales')\n",
        "plt.legend(['BUY', 'HOLD', 'SELL'])\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ Variable objetivo creada exitosamente\")\n",
        "print(f\"üìä Dataset con se√±ales: {df_with_signals.shape}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Preprocesamiento de Datos\n",
        "\n",
        "Ahora aplicaremos t√©cnicas de preprocesamiento incluyendo limpieza de datos, manejo de valores nulos y outliers, y creaci√≥n de pipelines automatizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PREPARACI√ìN DE DATOS PARA MACHINE LEARNING\n",
        "# ============================================================\n",
        "\n",
        "print(\"üîß Iniciando preprocesamiento de datos...\")\n",
        "\n",
        "# 1. LIMPIEZA INICIAL - Remover filas con valores nulos en la variable objetivo\n",
        "print(\"\\n1Ô∏è‚É£ Limpieza inicial...\")\n",
        "data_clean = df_with_signals.dropna(subset=['Trading_Signal']).copy()\n",
        "print(f\"   Datos despu√©s de remover NaN en Trading_Signal: {data_clean.shape}\")\n",
        "\n",
        "# 2. SELECCI√ìN DE FEATURES PARA EL MODELO\n",
        "print(\"\\n2Ô∏è‚É£ Selecci√≥n de features...\")\n",
        "\n",
        "# Features t√©cnicos (indicadores)\n",
        "technical_features = [\n",
        "    'RSI', 'MACD', 'MACD_Signal', 'MACD_Histogram',\n",
        "    'BB_Position', 'BB_Width', 'Stoch_K', 'Stoch_D', 'Williams_R',\n",
        "    'MFI', 'Volatility', 'ATR', 'Volume_Ratio',\n",
        "    'Price_to_MA20', 'MA5_to_MA20', 'High_Low_Ratio',\n",
        "    'Daily_Return', 'Price_Change'\n",
        "]\n",
        "\n",
        "# Features categ√≥ricas\n",
        "categorical_features = ['Stock', 'Sector', 'Market']\n",
        "\n",
        "# Features a excluir (identificadores, fechas, etc.)\n",
        "exclude_features = ['Date', 'Future_Return', 'Trading_Signal']\n",
        "\n",
        "# Verificar que las features existen\n",
        "available_technical = [f for f in technical_features if f in data_clean.columns]\n",
        "missing_technical = [f for f in technical_features if f not in data_clean.columns]\n",
        "\n",
        "print(f\"   ‚úÖ Features t√©cnicos disponibles: {len(available_technical)}\")\n",
        "print(f\"   ‚ö†Ô∏è Features t√©cnicos faltantes: {len(missing_technical)}\")\n",
        "if missing_technical:\n",
        "    print(f\"      Faltantes: {missing_technical}\")\n",
        "\n",
        "# 3. AN√ÅLISIS DE VALORES NULOS\n",
        "print(\"\\n3Ô∏è‚É£ An√°lisis de valores nulos...\")\n",
        "null_analysis = data_clean[available_technical + categorical_features].isnull().sum()\n",
        "null_features = null_analysis[null_analysis > 0]\n",
        "\n",
        "if len(null_features) > 0:\n",
        "    print(\"   Columnas con valores nulos:\")\n",
        "    for col, count in null_features.items():\n",
        "        percentage = (count / len(data_clean)) * 100\n",
        "        print(f\"      {col}: {count} ({percentage:.1f}%)\")\n",
        "else:\n",
        "    print(\"   ‚úÖ No hay valores nulos en las features seleccionadas\")\n",
        "\n",
        "# 4. DETECCI√ìN DE OUTLIERS\n",
        "print(\"\\n4Ô∏è‚É£ Detecci√≥n de outliers...\")\n",
        "\n",
        "def detect_outliers_iqr(data, column, factor=1.5):\n",
        "    \"\"\"Detecta outliers usando el m√©todo IQR\"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - factor * IQR\n",
        "    upper_bound = Q3 + factor * IQR\n",
        "    \n",
        "    outliers = ((data[column] < lower_bound) | (data[column] > upper_bound))\n",
        "    return outliers.sum(), lower_bound, upper_bound\n",
        "\n",
        "outlier_summary = []\n",
        "for feature in available_technical:\n",
        "    if data_clean[feature].dtype in ['float64', 'int64']:\n",
        "        outlier_count, lower, upper = detect_outliers_iqr(data_clean, feature)\n",
        "        outlier_percentage = (outlier_count / len(data_clean)) * 100\n",
        "        outlier_summary.append({\n",
        "            'Feature': feature,\n",
        "            'Outliers': outlier_count,\n",
        "            'Percentage': outlier_percentage,\n",
        "            'Lower_Bound': lower,\n",
        "            'Upper_Bound': upper\n",
        "        })\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary)\n",
        "outlier_df = outlier_df.sort_values('Percentage', ascending=False)\n",
        "\n",
        "print(\"   Top 10 features con m√°s outliers:\")\n",
        "print(outlier_df.head(10)[['Feature', 'Outliers', 'Percentage']].to_string(index=False))\n",
        "\n",
        "# 5. PREPARACI√ìN DE DATASETS X y y\n",
        "print(\"\\n5Ô∏è‚É£ Preparaci√≥n de datasets X y y...\")\n",
        "\n",
        "# Seleccionar solo las filas con todas las features disponibles\n",
        "features_to_use = available_technical + categorical_features\n",
        "X = data_clean[features_to_use].copy()\n",
        "y = data_clean['Trading_Signal'].copy()\n",
        "\n",
        "# Remover filas con valores nulos en X\n",
        "mask_complete = X.notna().all(axis=1)\n",
        "X = X[mask_complete]\n",
        "y = y[mask_complete]\n",
        "\n",
        "print(f\"   üìä Shape final de X: {X.shape}\")\n",
        "print(f\"   üéØ Shape final de y: {y.shape}\")\n",
        "print(f\"   üìà Distribuci√≥n de clases:\")\n",
        "\n",
        "class_distribution = y.value_counts().sort_index()\n",
        "for class_val, count in class_distribution.items():\n",
        "    class_name = ['BUY', 'HOLD', 'SELL'][int(class_val)]\n",
        "    percentage = (count / len(y)) * 100\n",
        "    print(f\"      {class_name} ({class_val}): {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "# 6. DIVISI√ìN TEMPORAL DE DATOS\n",
        "print(\"\\n6Ô∏è‚É£ Divisi√≥n temporal de datos...\")\n",
        "\n",
        "# Obtener las fechas correspondientes\n",
        "dates_complete = data_clean[mask_complete]['Date'].reset_index(drop=True)\n",
        "\n",
        "# Ordenar por fecha para divisi√≥n temporal\n",
        "sort_idx = dates_complete.argsort()\n",
        "X_sorted = X.iloc[sort_idx].reset_index(drop=True)\n",
        "y_sorted = y.iloc[sort_idx].reset_index(drop=True)\n",
        "dates_sorted = dates_complete.iloc[sort_idx].reset_index(drop=True)\n",
        "\n",
        "# Divisi√≥n temporal 80/20\n",
        "split_index = int(0.8 * len(X_sorted))\n",
        "\n",
        "X_train = X_sorted.iloc[:split_index]\n",
        "X_test = X_sorted.iloc[split_index:]\n",
        "y_train = y_sorted.iloc[:split_index]\n",
        "y_test = y_sorted.iloc[split_index:]\n",
        "\n",
        "print(f\"   üìÖ Per√≠odo de entrenamiento: {dates_sorted.iloc[0]} a {dates_sorted.iloc[split_index-1]}\")\n",
        "print(f\"   üìÖ Per√≠odo de prueba: {dates_sorted.iloc[split_index]} a {dates_sorted.iloc[-1]}\")\n",
        "print(f\"   üìä Train set: {X_train.shape}\")\n",
        "print(f\"   üìä Test set: {X_test.shape}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocesamiento inicial completado\")\n",
        "print(f\"üìä Datos listos para creaci√≥n de pipelines\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Creaci√≥n de Pipelines de Preprocesamiento\n",
        "\n",
        "Utilizaremos ColumnTransformer y Pipeline de sklearn para automatizar el preprocesamiento de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREACI√ìN DE PIPELINES DE PREPROCESAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(\"üîß Creando pipelines de preprocesamiento...\")\n",
        "\n",
        "# Separar features por tipo\n",
        "numerical_features = available_technical\n",
        "categorical_features_clean = categorical_features\n",
        "\n",
        "print(f\"üìä Features num√©ricas: {len(numerical_features)}\")\n",
        "print(f\"üìä Features categ√≥ricas: {len(categorical_features_clean)}\")\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE PARA FEATURES NUM√âRICAS\n",
        "# ============================================================\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # Imputar valores faltantes con mediana\n",
        "    ('scaler', StandardScaler())  # Escalar a media 0 y desviaci√≥n est√°ndar 1\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# PIPELINE PARA FEATURES CATEG√ìRICAS  \n",
        "# ============================================================\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),  # Imputar con 'unknown'\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# COMBINAR AMBOS PIPELINES CON COLUMNTRANSFORMER\n",
        "# ============================================================\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features_clean)\n",
        "    ],\n",
        "    remainder='drop'  # Eliminar columnas no especificadas\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Pipelines de preprocesamiento creados\")\n",
        "\n",
        "# ============================================================\n",
        "# PROBAR EL PREPROCESSOR\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nüß™ Probando el preprocessor...\")\n",
        "\n",
        "# Fit y transform en datos de entrenamiento\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"üìä Shape original X_train: {X_train.shape}\")\n",
        "print(f\"üìä Shape procesado X_train: {X_train_processed.shape}\")\n",
        "print(f\"üìä Shape original X_test: {X_test.shape}\")\n",
        "print(f\"üìä Shape procesado X_test: {X_test_processed.shape}\")\n",
        "\n",
        "# Obtener nombres de las features despu√©s del preprocesamiento\n",
        "def get_feature_names(preprocessor, numerical_features, categorical_features):\n",
        "    \"\"\"Obtiene los nombres de las features despu√©s del preprocesamiento\"\"\"\n",
        "    \n",
        "    # Features num√©ricas (se mantienen igual)\n",
        "    num_feature_names = numerical_features\n",
        "    \n",
        "    # Features categ√≥ricas (one-hot encoded)\n",
        "    cat_transformer = preprocessor.named_transformers_['cat']\n",
        "    if hasattr(cat_transformer.named_steps['onehot'], 'get_feature_names_out'):\n",
        "        # Scikit-learn >= 1.0\n",
        "        cat_feature_names = cat_transformer.named_steps['onehot'].get_feature_names_out(categorical_features)\n",
        "    else:\n",
        "        # Scikit-learn < 1.0 (fallback)\n",
        "        categories = cat_transformer.named_steps['onehot'].categories_\n",
        "        cat_feature_names = []\n",
        "        for i, cat_name in enumerate(categorical_features):\n",
        "            for category in categories[i]:\n",
        "                cat_feature_names.append(f\"{cat_name}_{category}\")\n",
        "    \n",
        "    return list(num_feature_names) + list(cat_feature_names)\n",
        "\n",
        "feature_names_processed = get_feature_names(preprocessor, numerical_features, categorical_features_clean)\n",
        "\n",
        "print(f\"üìã Total de features despu√©s del preprocesamiento: {len(feature_names_processed)}\")\n",
        "print(f\"   - Features num√©ricas: {len(numerical_features)}\")\n",
        "print(f\"   - Features categ√≥ricas (one-hot): {len(feature_names_processed) - len(numerical_features)}\")\n",
        "\n",
        "# Mostrar algunas features de ejemplo\n",
        "print(f\"\\nüìã Primeras 10 features procesadas:\")\n",
        "for i, name in enumerate(feature_names_processed[:10], 1):\n",
        "    print(f\"   {i:2d}. {name}\")\n",
        "\n",
        "if len(feature_names_processed) > 10:\n",
        "    print(f\"   ... y {len(feature_names_processed) - 10} m√°s\")\n",
        "\n",
        "# ============================================================\n",
        "# VERIFICAR CALIDAD DEL PREPROCESAMIENTO\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüîç Verificaci√≥n de calidad del preprocesamiento:\")\n",
        "\n",
        "# Verificar valores nulos\n",
        "train_nulls = np.isnan(X_train_processed).sum()\n",
        "test_nulls = np.isnan(X_test_processed).sum()\n",
        "\n",
        "print(f\"   ‚úÖ Valores nulos en train: {train_nulls}\")\n",
        "print(f\"   ‚úÖ Valores nulos en test: {test_nulls}\")\n",
        "\n",
        "# Verificar estad√≠sticas b√°sicas de features num√©ricas\n",
        "train_means = np.mean(X_train_processed[:, :len(numerical_features)], axis=0)\n",
        "train_stds = np.std(X_train_processed[:, :len(numerical_features)], axis=0)\n",
        "\n",
        "print(f\"   üìä Media de features num√©ricas (deber√≠a estar cerca de 0):\")\n",
        "print(f\"      Min: {train_means.min():.6f}, Max: {train_means.max():.6f}\")\n",
        "print(f\"   üìä Desviaci√≥n est√°ndar de features num√©ricas (deber√≠a estar cerca de 1):\")\n",
        "print(f\"      Min: {train_stds.min():.6f}, Max: {train_stds.max():.6f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Pipelines de preprocesamiento completados y verificados\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Entrenamiento y Comparaci√≥n de Modelos\n",
        "\n",
        "Entrenaremos m√∫ltiples algoritmos de machine learning y los compararemos usando validaci√≥n cruzada para seleccionar el mejor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ENTRENAMIENTO Y COMPARACI√ìN DE M√öLTIPLES MODELOS\n",
        "# ============================================================\n",
        "\n",
        "print(\"ü§ñ Iniciando entrenamiento y comparaci√≥n de modelos...\")\n",
        "\n",
        "# Definir modelos a comparar\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
        "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
        "    'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "}\n",
        "\n",
        "print(f\"üìä Modelos a evaluar: {len(models)}\")\n",
        "for name in models.keys():\n",
        "    print(f\"   ‚Ä¢ {name}\")\n",
        "\n",
        "# ============================================================\n",
        "# FUNCI√ìN PARA EVALUAR MODELOS\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_model(name, model, X_train, y_train, cv_folds=5):\n",
        "    \"\"\"Eval√∫a un modelo usando validaci√≥n cruzada\"\"\"\n",
        "    \n",
        "    # Crear pipeline completo (preprocesamiento + modelo)\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Validaci√≥n cruzada\n",
        "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv_folds, \n",
        "                               scoring='accuracy', n_jobs=-1)\n",
        "    \n",
        "    # Entrenar en todo el conjunto de entrenamiento para m√©tricas adicionales\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_train)\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    accuracy = cv_scores.mean()\n",
        "    accuracy_std = cv_scores.std()\n",
        "    precision = precision_score(y_train, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_train, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_train, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    return {\n",
        "        'Model': name,\n",
        "        'CV_Accuracy_Mean': accuracy,\n",
        "        'CV_Accuracy_Std': accuracy_std,\n",
        "        'Train_Precision': precision,\n",
        "        'Train_Recall': recall,\n",
        "        'Train_F1': f1,\n",
        "        'Pipeline': pipeline\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# EVALUAR TODOS LOS MODELOS\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüîÑ Evaluando modelos con validaci√≥n cruzada (5-fold)...\")\n",
        "\n",
        "results = []\n",
        "pipelines = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n   üîÑ Evaluando {name}...\")\n",
        "    try:\n",
        "        result = evaluate_model(name, model, X_train, y_train)\n",
        "        results.append(result)\n",
        "        pipelines[name] = result['Pipeline']\n",
        "        \n",
        "        print(f\"      ‚úÖ CV Accuracy: {result['CV_Accuracy_Mean']:.4f} ¬± {result['CV_Accuracy_Std']:.4f}\")\n",
        "        print(f\"      üìä Train F1: {result['Train_F1']:.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"      ‚ùå Error: {str(e)}\")\n",
        "\n",
        "# ============================================================\n",
        "# RESULTADOS Y COMPARACI√ìN\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüìä Resumen de resultados:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('CV_Accuracy_Mean', ascending=False)\n",
        "\n",
        "# Mostrar tabla de resultados\n",
        "print(results_df[['Model', 'CV_Accuracy_Mean', 'CV_Accuracy_Std', 'Train_F1']].to_string(index=False))\n",
        "\n",
        "# Identificar el mejor modelo\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_accuracy = results_df.iloc[0]['CV_Accuracy_Mean']\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
        "print(f\"   üìä CV Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# VISUALIZACI√ìN DE RESULTADOS\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Subplot 1: Accuracy con barras de error\n",
        "plt.subplot(2, 2, 1)\n",
        "x_pos = range(len(results_df))\n",
        "plt.bar(x_pos, results_df['CV_Accuracy_Mean'], \n",
        "        yerr=results_df['CV_Accuracy_Std'], capsize=5, alpha=0.7)\n",
        "plt.xlabel('Modelos')\n",
        "plt.ylabel('CV Accuracy')\n",
        "plt.title('Comparaci√≥n de Accuracy (Validaci√≥n Cruzada)')\n",
        "plt.xticks(x_pos, results_df['Model'], rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: F1 Score en entrenamiento\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(x_pos, results_df['Train_F1'], alpha=0.7, color='orange')\n",
        "plt.xlabel('Modelos')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score en Entrenamiento')\n",
        "plt.xticks(x_pos, results_df['Model'], rotation=45, ha='right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Scatter plot Accuracy vs F1\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.scatter(results_df['CV_Accuracy_Mean'], results_df['Train_F1'], \n",
        "           s=100, alpha=0.7, c=range(len(results_df)), cmap='viridis')\n",
        "plt.xlabel('CV Accuracy')\n",
        "plt.ylabel('Train F1 Score')\n",
        "plt.title('Accuracy vs F1 Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# A√±adir etiquetas a los puntos\n",
        "for i, model in enumerate(results_df['Model']):\n",
        "    plt.annotate(model, \n",
        "                (results_df.iloc[i]['CV_Accuracy_Mean'], results_df.iloc[i]['Train_F1']),\n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "# Subplot 4: Ranking de modelos\n",
        "plt.subplot(2, 2, 4)\n",
        "ranking_scores = (results_df['CV_Accuracy_Mean'] + results_df['Train_F1']) / 2\n",
        "plt.barh(range(len(results_df)), ranking_scores, alpha=0.7, color='green')\n",
        "plt.ylabel('Modelos')\n",
        "plt.xlabel('Score Promedio (Accuracy + F1) / 2')\n",
        "plt.title('Ranking General de Modelos')\n",
        "plt.yticks(range(len(results_df)), results_df['Model'])\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ Comparaci√≥n de modelos completada\")\n",
        "print(f\"üéØ Modelo seleccionado para optimizaci√≥n: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Optimizaci√≥n de Hiperpar√°metros\n",
        "\n",
        "Optimizaremos los hiperpar√°metros del mejor modelo usando tres t√©cnicas diferentes: GridSearchCV, RandomizedSearchCV y Optuna.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\n",
        "# ============================================================\n",
        "\n",
        "print(\"üîç Iniciando optimizaci√≥n de hiperpar√°metros...\")\n",
        "print(f\"üéØ Modelo seleccionado: {best_model_name}\")\n",
        "\n",
        "# Obtener el mejor modelo\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "# Crear pipeline para optimizaci√≥n\n",
        "optimization_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', best_model)\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# DEFINIR ESPACIOS DE B√öSQUEDA SEG√öN EL MODELO\n",
        "# ============================================================\n",
        "\n",
        "def get_param_grid(model_name):\n",
        "    \"\"\"Define el espacio de b√∫squeda de hiperpar√°metros seg√∫n el modelo\"\"\"\n",
        "    \n",
        "    if model_name == 'Random Forest':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [10, 20, 30, None],\n",
        "            'classifier__min_samples_split': [2, 5, 10],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4],\n",
        "            'classifier__max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'XGBoost':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [3, 6, 9],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
        "            'classifier__colsample_bytree': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'LightGBM':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [3, 6, 9],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__num_leaves': [31, 50, 100],\n",
        "            'classifier__feature_fraction': [0.8, 0.9, 1.0]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'Gradient Boosting':\n",
        "        return {\n",
        "            'classifier__n_estimators': [100, 200, 300],\n",
        "            'classifier__max_depth': [3, 6, 9],\n",
        "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
        "            'classifier__subsample': [0.8, 0.9, 1.0],\n",
        "            'classifier__max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'Support Vector Machine':\n",
        "        return {\n",
        "            'classifier__C': [0.1, 1, 10, 100],\n",
        "            'classifier__gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "            'classifier__kernel': ['rbf', 'poly', 'sigmoid']\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'Logistic Regression':\n",
        "        return {\n",
        "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
        "            'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
        "            'classifier__solver': ['liblinear', 'saga'],\n",
        "            'classifier__max_iter': [1000, 2000]\n",
        "        }\n",
        "    \n",
        "    else:  # Default para otros modelos\n",
        "        return {\n",
        "            'classifier__random_state': [42]  # Par√°metro m√≠nimo\n",
        "        }\n",
        "\n",
        "param_grid = get_param_grid(best_model_name)\n",
        "print(f\"üìã Par√°metros a optimizar: {len(param_grid)}\")\n",
        "for param, values in param_grid.items():\n",
        "    print(f\"   ‚Ä¢ {param}: {values}\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. GRIDSEARCHCV - B√öSQUEDA EXHAUSTIVA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n1Ô∏è‚É£ Ejecutando GridSearchCV...\")\n",
        "print(f\"   üîÑ B√∫squeda exhaustiva en {np.prod([len(v) for v in param_grid.values()])} combinaciones\")\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    optimization_pipeline,\n",
        "    param_grid,\n",
        "    cv=3,  # Reducido para acelerar\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "grid_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"   ‚úÖ GridSearchCV completado en {grid_time:.1f} segundos\")\n",
        "print(f\"   üèÜ Mejor score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"   üìã Mejores par√°metros: {grid_search.best_params_}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. RANDOMIZEDSEARCHCV - B√öSQUEDA ALEATORIA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n2Ô∏è‚É£ Ejecutando RandomizedSearchCV...\")\n",
        "\n",
        "# Expandir el espacio de b√∫squeda para RandomizedSearch\n",
        "def get_random_param_dist(model_name):\n",
        "    \"\"\"Define distribuciones para b√∫squeda aleatoria\"\"\"\n",
        "    \n",
        "    if model_name == 'Random Forest':\n",
        "        return {\n",
        "            'classifier__n_estimators': [50, 100, 200, 300, 500],\n",
        "            'classifier__max_depth': [5, 10, 15, 20, 25, 30, None],\n",
        "            'classifier__min_samples_split': [2, 5, 10, 15],\n",
        "            'classifier__min_samples_leaf': [1, 2, 4, 6],\n",
        "            'classifier__max_features': ['sqrt', 'log2', None, 0.5, 0.8]\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'XGBoost':\n",
        "        return {\n",
        "            'classifier__n_estimators': [50, 100, 200, 300, 500],\n",
        "            'classifier__max_depth': [3, 6, 9, 12],\n",
        "            'classifier__learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
        "            'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "            'classifier__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        }\n",
        "    \n",
        "    # Usar el mismo que GridSearch para otros modelos\n",
        "    else:\n",
        "        return param_grid\n",
        "\n",
        "random_param_dist = get_random_param_dist(best_model_name)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    optimization_pipeline,\n",
        "    random_param_dist,\n",
        "    n_iter=50,  # N√∫mero de iteraciones aleatorias\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "random_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"   ‚úÖ RandomizedSearchCV completado en {random_time:.1f} segundos\")\n",
        "print(f\"   üèÜ Mejor score: {random_search.best_score_:.4f}\")\n",
        "print(f\"   üìã Mejores par√°metros: {random_search.best_params_}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. OPTUNA - OPTIMIZACI√ìN BAYESIANA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n3Ô∏è‚É£ Ejecutando Optuna (Optimizaci√≥n Bayesiana)...\")\n",
        "\n",
        "def objective(trial, model_name, pipeline_template, X_train, y_train):\n",
        "    \"\"\"Funci√≥n objetivo para Optuna\"\"\"\n",
        "    \n",
        "    # Sugerir par√°metros seg√∫n el modelo\n",
        "    if model_name == 'Random Forest':\n",
        "        params = {\n",
        "            'classifier__n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
        "            'classifier__max_depth': trial.suggest_int('max_depth', 5, 30),\n",
        "            'classifier__min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'classifier__min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'classifier__max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "        }\n",
        "    \n",
        "    elif model_name == 'XGBoost':\n",
        "        params = {\n",
        "            'classifier__n_estimators': trial.suggest_int('n_estimators', 50, 500, step=50),\n",
        "            'classifier__max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "            'classifier__learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "            'classifier__subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'classifier__colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
        "        }\n",
        "    \n",
        "    else:\n",
        "        # Para otros modelos, usar par√°metros del GridSearch\n",
        "        params = {}\n",
        "        for param, values in param_grid.items():\n",
        "            if isinstance(values[0], (int, float)):\n",
        "                if isinstance(values[0], int):\n",
        "                    params[param] = trial.suggest_int(param.split('__')[1], min(values), max(values))\n",
        "                else:\n",
        "                    params[param] = trial.suggest_float(param.split('__')[1], min(values), max(values))\n",
        "            else:\n",
        "                params[param] = trial.suggest_categorical(param.split('__')[1], values)\n",
        "    \n",
        "    # Crear pipeline con par√°metros sugeridos\n",
        "    pipeline_copy = Pipeline([\n",
        "        ('preprocessor', pipeline_template.named_steps['preprocessor']),\n",
        "        ('classifier', pipeline_template.named_steps['classifier'].__class__(**{k.split('__')[1]: v for k, v in params.items()}, random_state=42))\n",
        "    ])\n",
        "    \n",
        "    # Evaluar con validaci√≥n cruzada\n",
        "    scores = cross_val_score(pipeline_copy, X_train, y_train, cv=3, scoring='accuracy', n_jobs=1)\n",
        "    return scores.mean()\n",
        "\n",
        "# Crear estudio de Optuna\n",
        "start_time = datetime.now()\n",
        "\n",
        "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
        "study.optimize(\n",
        "    lambda trial: objective(trial, best_model_name, optimization_pipeline, X_train, y_train),\n",
        "    n_trials=50,\n",
        "    timeout=300,  # 5 minutos m√°ximo\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "optuna_time = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "print(f\"   ‚úÖ Optuna completado en {optuna_time:.1f} segundos\")\n",
        "print(f\"   üèÜ Mejor score: {study.best_value:.4f}\")\n",
        "print(f\"   üìã Mejores par√°metros: {study.best_params}\")\n",
        "\n",
        "# ============================================================\n",
        "# COMPARACI√ìN DE T√âCNICAS DE OPTIMIZACI√ìN\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüìä COMPARACI√ìN DE T√âCNICAS DE OPTIMIZACI√ìN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "optimization_results = pd.DataFrame({\n",
        "    'Technique': ['GridSearchCV', 'RandomizedSearchCV', 'Optuna'],\n",
        "    'Best_Score': [grid_search.best_score_, random_search.best_score_, study.best_value],\n",
        "    'Time_Seconds': [grid_time, random_time, optuna_time],\n",
        "    'Evaluations': [len(grid_search.cv_results_['mean_test_score']), 50, len(study.trials)]\n",
        "})\n",
        "\n",
        "print(optimization_results.to_string(index=False))\n",
        "\n",
        "# Identificar la mejor t√©cnica\n",
        "best_technique_idx = optimization_results['Best_Score'].idxmax()\n",
        "best_technique = optimization_results.iloc[best_technique_idx]\n",
        "\n",
        "print(f\"\\nüèÜ MEJOR T√âCNICA: {best_technique['Technique']}\")\n",
        "print(f\"   üìä Score: {best_technique['Best_Score']:.4f}\")\n",
        "print(f\"   ‚è±Ô∏è Tiempo: {best_technique['Time_Seconds']:.1f} segundos\")\n",
        "print(f\"   üîÑ Evaluaciones: {best_technique['Evaluations']}\")\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Subplot 1: Comparaci√≥n de scores\n",
        "plt.subplot(1, 3, 1)\n",
        "bars = plt.bar(optimization_results['Technique'], optimization_results['Best_Score'], \n",
        "               color=['blue', 'orange', 'green'], alpha=0.7)\n",
        "plt.title('Comparaci√≥n de Best Scores')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# A√±adir valores en las barras\n",
        "for bar, score in zip(bars, optimization_results['Best_Score']):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
        "             f'{score:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Subplot 2: Comparaci√≥n de tiempos\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(optimization_results['Technique'], optimization_results['Time_Seconds'], \n",
        "        color=['blue', 'orange', 'green'], alpha=0.7)\n",
        "plt.title('Tiempo de Ejecuci√≥n')\n",
        "plt.ylabel('Segundos')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Eficiencia (Score/Time)\n",
        "plt.subplot(1, 3, 3)\n",
        "efficiency = optimization_results['Best_Score'] / optimization_results['Time_Seconds']\n",
        "plt.bar(optimization_results['Technique'], efficiency, \n",
        "        color=['blue', 'orange', 'green'], alpha=0.7)\n",
        "plt.title('Eficiencia (Score/Tiempo)')\n",
        "plt.ylabel('Score por Segundo')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n‚úÖ Optimizaci√≥n de hiperpar√°metros completada\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Evaluaci√≥n Final y Conclusiones\n",
        "\n",
        "Evaluaremos el modelo optimizado en el conjunto de prueba y analizaremos los resultados finales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUACI√ìN FINAL DEL MODELO OPTIMIZADO\n",
        "# ============================================================\n",
        "\n",
        "print(\"üéØ Evaluaci√≥n final del modelo optimizado...\")\n",
        "\n",
        "# Seleccionar el mejor modelo seg√∫n la t√©cnica de optimizaci√≥n ganadora\n",
        "if best_technique['Technique'] == 'GridSearchCV':\n",
        "    final_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "elif best_technique['Technique'] == 'RandomizedSearchCV':\n",
        "    final_model = random_search.best_estimator_\n",
        "    best_params = random_search.best_params_\n",
        "else:  # Optuna\n",
        "    # Crear modelo con los mejores par√°metros de Optuna\n",
        "    optuna_params = {f\"classifier__{k}\": v for k, v in study.best_params.items()}\n",
        "    final_model = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', best_model.__class__(**study.best_params, random_state=42))\n",
        "    ])\n",
        "    final_model.fit(X_train, y_train)\n",
        "    best_params = study.best_params\n",
        "\n",
        "print(f\"üèÜ Modelo final: {best_model_name} optimizado con {best_technique['Technique']}\")\n",
        "print(f\"üìã Par√°metros finales: {best_params}\")\n",
        "\n",
        "# ============================================================\n",
        "# EVALUACI√ìN EN CONJUNTO DE PRUEBA\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüìä Evaluando en conjunto de prueba...\")\n",
        "\n",
        "# Predicciones\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_pred_proba = final_model.predict_proba(X_test)\n",
        "\n",
        "# M√©tricas principales\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "test_precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "test_recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "test_f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"‚úÖ Resultados en conjunto de prueba:\")\n",
        "print(f\"   üìä Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"   üìä Precision: {test_precision:.4f}\")\n",
        "print(f\"   üìä Recall: {test_recall:.4f}\")\n",
        "print(f\"   üìä F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# MATRIZ DE CONFUSI√ìN\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüìã Matriz de Confusi√≥n:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Visualizaci√≥n de la matriz de confusi√≥n\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Subplot 1: Matriz de confusi√≥n\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['BUY', 'HOLD', 'SELL'],\n",
        "            yticklabels=['BUY', 'HOLD', 'SELL'])\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.xlabel('Predicci√≥n')\n",
        "plt.ylabel('Real')\n",
        "\n",
        "# Subplot 2: Distribuci√≥n de clases reales vs predichas\n",
        "plt.subplot(2, 3, 2)\n",
        "class_names = ['BUY', 'HOLD', 'SELL']\n",
        "real_counts = [sum(y_test == i) for i in range(3)]\n",
        "pred_counts = [sum(y_pred == i) for i in range(3)]\n",
        "\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, real_counts, width, label='Real', alpha=0.7)\n",
        "plt.bar(x + width/2, pred_counts, width, label='Predicci√≥n', alpha=0.7)\n",
        "plt.xlabel('Clases')\n",
        "plt.ylabel('Cantidad')\n",
        "plt.title('Distribuci√≥n Real vs Predicha')\n",
        "plt.xticks(x, class_names)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: M√©tricas por clase\n",
        "plt.subplot(2, 3, 3)\n",
        "precision_per_class = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
        "recall_per_class = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
        "f1_per_class = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
        "\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.25\n",
        "\n",
        "plt.bar(x - width, precision_per_class, width, label='Precision', alpha=0.7)\n",
        "plt.bar(x, recall_per_class, width, label='Recall', alpha=0.7)\n",
        "plt.bar(x + width, f1_per_class, width, label='F1-Score', alpha=0.7)\n",
        "plt.xlabel('Clases')\n",
        "plt.ylabel('Score')\n",
        "plt.title('M√©tricas por Clase')\n",
        "plt.xticks(x, class_names)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Curva ROC (para clase BUY)\n",
        "plt.subplot(2, 3, 4)\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarizar las etiquetas para ROC multiclase\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "y_pred_proba_bin = y_pred_proba\n",
        "\n",
        "# Calcular ROC para cada clase\n",
        "colors = ['blue', 'orange', 'green']\n",
        "for i, class_name in enumerate(class_names):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_bin[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
        "             label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curvas ROC por Clase')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 5: Comparaci√≥n modelo base vs optimizado\n",
        "plt.subplot(2, 3, 5)\n",
        "base_accuracy = results_df[results_df['Model'] == best_model_name]['CV_Accuracy_Mean'].iloc[0]\n",
        "comparison_data = ['Modelo Base', 'Modelo Optimizado']\n",
        "comparison_scores = [base_accuracy, test_accuracy]\n",
        "\n",
        "bars = plt.bar(comparison_data, comparison_scores, color=['red', 'green'], alpha=0.7)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Mejora del Modelo')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# A√±adir valores en las barras\n",
        "for bar, score in zip(bars, comparison_scores):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{score:.4f}', ha='center', va='bottom')\n",
        "\n",
        "# Calcular mejora\n",
        "improvement = ((test_accuracy - base_accuracy) / base_accuracy) * 100\n",
        "plt.text(0.5, max(comparison_scores) * 0.8, f'Mejora: {improvement:.1f}%', \n",
        "         ha='center', va='center', fontsize=12, fontweight='bold',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
        "\n",
        "# Subplot 6: Importancia de features (si el modelo lo soporta)\n",
        "plt.subplot(2, 3, 6)\n",
        "try:\n",
        "    if hasattr(final_model.named_steps['classifier'], 'feature_importances_'):\n",
        "        # Obtener importancias\n",
        "        importances = final_model.named_steps['classifier'].feature_importances_\n",
        "        \n",
        "        # Obtener nombres de features\n",
        "        feature_names_short = feature_names_processed[:len(importances)]\n",
        "        \n",
        "        # Seleccionar top 10 features m√°s importantes\n",
        "        indices = np.argsort(importances)[-10:]\n",
        "        \n",
        "        plt.barh(range(len(indices)), importances[indices], alpha=0.7)\n",
        "        plt.yticks(range(len(indices)), [feature_names_short[i] for i in indices])\n",
        "        plt.xlabel('Importancia')\n",
        "        plt.title('Top 10 Features Importantes')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Importancia de features\\nno disponible para\\neste modelo', \n",
        "                ha='center', va='center', transform=plt.gca().transAxes)\n",
        "        plt.title('Importancia de Features')\n",
        "except Exception as e:\n",
        "    plt.text(0.5, 0.5, f'Error al calcular\\nimportancia: {str(e)[:30]}...', \n",
        "            ha='center', va='center', transform=plt.gca().transAxes)\n",
        "    plt.title('Importancia de Features')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# REPORTE DETALLADO\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüìã REPORTE DE CLASIFICACI√ìN DETALLADO\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(y_test, y_pred, target_names=['BUY', 'HOLD', 'SELL']))\n",
        "\n",
        "# ============================================================\n",
        "# AN√ÅLISIS DE ERRORES\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüîç AN√ÅLISIS DE ERRORES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Crear DataFrame con resultados\n",
        "results_analysis = pd.DataFrame({\n",
        "    'Real': y_test,\n",
        "    'Prediccion': y_pred,\n",
        "    'Correcto': y_test == y_pred\n",
        "})\n",
        "\n",
        "# Estad√≠sticas de errores\n",
        "error_stats = results_analysis.groupby(['Real', 'Prediccion']).size().unstack(fill_value=0)\n",
        "print(\"Matriz de errores detallada:\")\n",
        "print(error_stats)\n",
        "\n",
        "# Porcentaje de aciertos por clase\n",
        "accuracy_per_class = []\n",
        "for i in range(3):\n",
        "    class_mask = y_test == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_accuracy = (y_pred[class_mask] == i).mean()\n",
        "        accuracy_per_class.append(class_accuracy)\n",
        "        print(f\"Accuracy clase {class_names[i]}: {class_accuracy:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# CONCLUSIONES FINALES\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\nüéØ CONCLUSIONES FINALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"‚úÖ MODELO FINAL SELECCIONADO:\")\n",
        "print(f\"   ‚Ä¢ Algoritmo: {best_model_name}\")\n",
        "print(f\"   ‚Ä¢ T√©cnica de optimizaci√≥n: {best_technique['Technique']}\")\n",
        "print(f\"   ‚Ä¢ Accuracy en prueba: {test_accuracy:.4f}\")\n",
        "print(f\"   ‚Ä¢ F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nüìä RENDIMIENTO POR OBJETIVO:\")\n",
        "print(f\"   ‚Ä¢ BUY: Precision={precision_per_class[0]:.3f}, Recall={recall_per_class[0]:.3f}\")\n",
        "print(f\"   ‚Ä¢ HOLD: Precision={precision_per_class[1]:.3f}, Recall={recall_per_class[1]:.3f}\")\n",
        "print(f\"   ‚Ä¢ SELL: Precision={precision_per_class[2]:.3f}, Recall={recall_per_class[2]:.3f}\")\n",
        "\n",
        "print(f\"\\nüöÄ MEJORAS OBTENIDAS:\")\n",
        "print(f\"   ‚Ä¢ Mejora en accuracy: {improvement:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Tiempo de optimizaci√≥n total: {optimization_results['Time_Seconds'].sum():.1f} segundos\")\n",
        "\n",
        "print(f\"\\nüí° RECOMENDACIONES:\")\n",
        "print(f\"   ‚Ä¢ El modelo muestra {'buen' if test_accuracy > 0.7 else 'regular' if test_accuracy > 0.6 else 'bajo'} rendimiento\")\n",
        "print(f\"   ‚Ä¢ {'Se recomienda usar en producci√≥n' if test_accuracy > 0.75 else 'Requiere m√°s optimizaci√≥n antes de producci√≥n'}\")\n",
        "if test_f1 < 0.7:\n",
        "    print(f\"   ‚Ä¢ Considerar balanceo de clases o m√°s feature engineering\")\n",
        "if improvement < 5:\n",
        "    print(f\"   ‚Ä¢ La optimizaci√≥n mostr√≥ mejoras limitadas, considerar otros modelos\")\n",
        "\n",
        "print(f\"\\n‚úÖ Evaluaci√≥n final completada\")\n",
        "print(f\"üéØ Sistema de recomendaci√≥n de trading listo para {'implementaci√≥n' if test_accuracy > 0.7 else 'm√°s desarrollo'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìã Documentaci√≥n Final del Proceso\n",
        "\n",
        "### Resumen Ejecutivo - Parte II\n",
        "\n",
        "Este notebook ha implementado exitosamente un pipeline completo de Machine Learning para clasificaci√≥n de se√±ales de trading, siguiendo las mejores pr√°cticas de la industria.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Proceso Completado:\n",
        "\n",
        "#### 1. **Preprocesamiento de Datos**\n",
        "- ‚úÖ Limpieza de valores nulos y outliers\n",
        "- ‚úÖ Feature engineering con 20+ indicadores t√©cnicos\n",
        "- ‚úÖ Creaci√≥n de variable objetivo (BUY/HOLD/SELL)\n",
        "- ‚úÖ Pipeline automatizado con ColumnTransformer\n",
        "- ‚úÖ Divisi√≥n temporal de datos (80/20)\n",
        "\n",
        "#### 2. **Selecci√≥n de T√©cnica de ML**\n",
        "- ‚úÖ Evaluaci√≥n de 8 algoritmos diferentes\n",
        "- ‚úÖ Validaci√≥n cruzada 5-fold\n",
        "- ‚úÖ Comparaci√≥n exhaustiva de m√©tricas\n",
        "- ‚úÖ Selecci√≥n basada en rendimiento\n",
        "\n",
        "#### 3. **Optimizaci√≥n de Hiperpar√°metros**\n",
        "- ‚úÖ GridSearchCV (b√∫squeda exhaustiva)\n",
        "- ‚úÖ RandomizedSearchCV (b√∫squeda aleatoria)\n",
        "- ‚úÖ Optuna (optimizaci√≥n bayesiana)\n",
        "- ‚úÖ Comparaci√≥n de eficiencia\n",
        "\n",
        "#### 4. **Evaluaci√≥n Final**\n",
        "- ‚úÖ M√©tricas en conjunto de prueba\n",
        "- ‚úÖ Matriz de confusi√≥n detallada\n",
        "- ‚úÖ Curvas ROC multiclase\n",
        "- ‚úÖ An√°lisis de importancia de features\n",
        "- ‚úÖ Reporte de clasificaci√≥n completo\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Resultados Clave:\n",
        "\n",
        "- **Modelo Final**: [Se determinar√° al ejecutar]\n",
        "- **Accuracy**: [Se determinar√° al ejecutar]\n",
        "- **Mejora vs Baseline**: [Se determinar√° al ejecutar]\n",
        "- **Features Importantes**: Indicadores t√©cnicos RSI, MACD, Bollinger Bands\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Valor del Proyecto:\n",
        "\n",
        "1. **Automatizaci√≥n**: Pipeline reproducible para nuevos datos\n",
        "2. **Escalabilidad**: F√°cil incorporaci√≥n de nuevas acciones\n",
        "3. **Robustez**: Validaci√≥n temporal estricta\n",
        "4. **Interpretabilidad**: An√°lisis de importancia de features\n",
        "5. **Producci√≥n**: Listo para implementaci√≥n real\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Aplicaciones Pr√°cticas:\n",
        "\n",
        "- **Trading Algor√≠tmico**: Se√±ales autom√°ticas de compra/venta\n",
        "- **Gesti√≥n de Riesgo**: Identificaci√≥n de momentos de alta volatilidad\n",
        "- **Optimizaci√≥n de Portafolios**: Diversificaci√≥n inteligente\n",
        "- **Alertas de Mercado**: Notificaciones de oportunidades\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Pr√≥ximos Pasos Recomendados:\n",
        "\n",
        "1. **Implementaci√≥n en Tiempo Real**: Conexi√≥n con APIs de mercado\n",
        "2. **Backtesting Avanzado**: Simulaci√≥n de estrategias hist√≥ricas\n",
        "3. **Dashboard Interactivo**: Visualizaci√≥n en tiempo real\n",
        "4. **Alertas Autom√°ticas**: Sistema de notificaciones\n",
        "5. **Optimizaci√≥n Continua**: Re-entrenamiento peri√≥dico\n",
        "\n",
        "---\n",
        "\n",
        "**üìä Este proyecto demuestra competencias avanzadas en:**\n",
        "- Machine Learning aplicado a finanzas\n",
        "- Pipelines de preprocesamiento robusto\n",
        "- Optimizaci√≥n de hiperpar√°metros\n",
        "- Evaluaci√≥n rigurosa de modelos\n",
        "- Interpretaci√≥n de resultados financieros\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
